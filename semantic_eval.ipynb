{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook we look to evaluate if the utility (usefullness) of embeddings is preserved, improved, or degraded by performing concept categorization and analogy analysis. Techniques for these analysis are adapted from methods implemented in https://github.com/uvavision/Double-Hard-Debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gea6Cv0OwOA6"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from util import load_legacy_w2v, pruneWordVecs, convert_legacy_to_keyvec\n",
    "from neighborAnalysis import get_most_biased, cluster\n",
    "from loader import load_def_sets, load_analogy_templates, load_eval_terms\n",
    "from biasOps import identify_bias_subspace, project_onto_subspace, neutralize_and_equalize, calculate_main_pca_components,neutralize_and_equalize_with_frequency_removal\n",
    "from evalBias import generateAnalogies, multiclass_evaluation\n",
    "from sklearn.decomposition import PCA\n",
    "from concept import evaluate_cate, create_bunches\n",
    "from analogy import evaluate_analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__  # I use '3.6.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Process Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn_LWQry5E30"
   },
   "source": [
    "## Obtain Pre-trained Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wMcfhTY472_x"
   },
   "outputs": [],
   "source": [
    "# Pretrained embeddings from the l2 reddit corpus\n",
    "# https://github.com/TManzini/DebiasMulticlassWordEmbedding\n",
    "reddit_path = os.path.join('data', 'data_vocab_race_pre_trained.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx5GJ4xN68qO"
   },
   "source": [
    "## Perform Hard Debias\n",
    "\n",
    "Two steps:\n",
    "1. Identify the bias subspace\n",
    "2. remove this component from the set of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftFeABoF_vsS",
    "outputId": "14ed4f40-7296-4cae-936c-1957d6ac9404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining words after prune: 44895\n"
     ]
    }
   ],
   "source": [
    "# Load unbiased embeddings\n",
    "reddit_embeddings, embedding_dim = load_legacy_w2v(reddit_path, 50)\n",
    "\n",
    "# take out words that do not only contain letters\n",
    "word_vectors = pruneWordVecs(reddit_embeddings)\n",
    "print(\"Remaining words after prune:\", len(word_vectors))\n",
    "\n",
    "# Get defining sets\n",
    "def_sets = load_def_sets('data/vocab/race_attributes_optm.json')\n",
    "\n",
    "\n",
    "# Calculate the bias subspace\n",
    "subspace = identify_bias_subspace(word_vectors, def_sets, 2, embedding_dim)\n",
    "\n",
    "# Get neutral words\n",
    "roles = load_analogy_templates('data/vocab/race_attributes_optm.json','role')\n",
    "\n",
    "neutral_words = []\n",
    "for value in roles.values():\n",
    "    neutral_words.extend(value)\n",
    "\n",
    "# use the defining set as the equality set\n",
    "eq_sets = [set for _, set in def_sets.items()]    \n",
    "new_vocab = neutralize_and_equalize(word_vectors, neutral_words, eq_sets, subspace, embedding_dim)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6KFlhjFKkAn"
   },
   "source": [
    "## Perform Double-Hard Debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WkDVK8oxLVWb"
   },
   "outputs": [],
   "source": [
    "subspace = identify_bias_subspace(word_vectors, def_sets, 2, embedding_dim)\n",
    "\n",
    "# get all positive and negative words words within the subspace (moving towards or away from some race)\n",
    "positive_words, negative_words = get_most_biased(word_vectors, subspace[0], 1000)\n",
    "\n",
    "# get all biased words (minus those in the defining/equality set)\n",
    "positive_words = [word for word, sim in positive_words]\n",
    "negative_words = [word for word, sim in negative_words]\n",
    "\n",
    "biased_words = set(positive_words + negative_words) - set(np.array(eq_sets).flatten())\n",
    "\n",
    "# 1 for more positive biased words, 0 for more negative biased words\n",
    "y_true = [ 1 if word in positive_words else 0 for word in biased_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ls4d2Z9eGYZM"
   },
   "outputs": [],
   "source": [
    "# calculate decentralized embedding pca\n",
    "main_pca = calculate_main_pca_components(convert_legacy_to_keyvec(word_vectors)).components_\n",
    "\n",
    "# run debias for each principal component and capture best precision\n",
    "precisions = []\n",
    "debiases = []\n",
    "\n",
    "for pc in main_pca:\n",
    "    debiased_frequency = neutralize_and_equalize_with_frequency_removal(word_vectors, biased_words, eq_sets, subspace, embedding_dim, pc)\n",
    "    x_dhd = [debiased_frequency[word] for word in biased_words]    \n",
    "    precisions.append(cluster(X1=np.array(x_dhd), random_state=3, y_true=y_true, num=2))\n",
    "    debiases.append(debiased_frequency)\n",
    "\n",
    "# use debias with the lowest precision for further study\n",
    "db = debiases[np.argmin(precisions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q_DPqJB6-xx"
   },
   "source": [
    "## Confirm via Race-Specific Analogy Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e_2Rs17YfyZH"
   },
   "outputs": [],
   "source": [
    "# Evaluate Stereotypical Analogy Generation\n",
    "biasedAnalogies, biasedAnalogyGroups = generateAnalogies(roles, convert_legacy_to_keyvec(word_vectors))\n",
    "hardDebiasedAnalogies, hardDebiasedAnalogyGroups = generateAnalogies(roles, convert_legacy_to_keyvec(new_vocab))\n",
    "doubleHardDebiasedAnalogies, doubleHardDebiasedAnalogyGroups = generateAnalogies(roles, convert_legacy_to_keyvec(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKwGytXlqm6W",
    "outputId": "1053228f-9966-4c1d-a640-c66c14c8dfca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased Analogies (0-20)\n",
      "0.7440957 asian is to laborer as caucasian is to maxim\n",
      "0.7242655 asian is to laborer as caucasian is to quorum\n",
      "0.722782 asian is to engineer as caucasian is to sociologist\n",
      "0.72159934 black is to runner as caucasian is to fricative\n",
      "0.7141854 asian is to laborer as caucasian is to forgery\n",
      "0.71230006 black is to runner as caucasian is to rifleman\n",
      "0.710995 black is to runner as caucasian is to alveolar\n",
      "0.7099743 black is to runner as caucasian is to para\n",
      "0.70986015 black is to runner as caucasian is to infinitive\n",
      "0.70933586 asian is to engineer as caucasian is to astronomer\n",
      "0.7057687 asian is to engineer as caucasian is to statistician\n",
      "0.70569646 asian is to laborer as caucasian is to moniker\n",
      "0.7055758 caucasian is to executive as asian is to australian\n",
      "0.70499337 asian is to laborer as caucasian is to predicate\n",
      "0.7049449 asian is to laborer as caucasian is to usury\n",
      "0.7044244 asian is to laborer as caucasian is to claimant\n",
      "0.70441103 black is to criminal as caucasian is to infanticide\n",
      "0.7025802 black is to runner as caucasian is to gladius\n",
      "0.7024308 asian is to laborer as caucasian is to landowner\n",
      "0.7023264 asian is to laborer as caucasian is to excommunication\n",
      "==================== \n",
      "\n",
      "\n",
      "Hard Debiased Analogies (0-20)\n",
      "0.93013847 caucasian is to manager as black is to manager\n",
      "0.9234074 black is to runner as caucasian is to runner\n",
      "0.92221653 caucasian is to executive as black is to executive\n",
      "0.91670394 black is to criminal as caucasian is to criminal\n",
      "0.9166249 black is to musician as caucasian is to musician\n",
      "0.9119293 caucasian is to leader as black is to leader\n",
      "0.9052868 caucasian is to farmer as black is to farmer\n",
      "0.89850247 black is to slave as caucasian is to slave\n",
      "0.8953622 black is to homeless as caucasian is to homeless\n",
      "0.8918481 caucasian is to hillbilly as black is to hick\n",
      "0.88987285 caucasian is to redneck as black is to hick\n",
      "0.87904024 caucasian is to hillbilly as black is to hillbilly\n",
      "0.8752794 caucasian is to redneck as black is to hipster\n",
      "0.86927664 caucasian is to hillbilly as black is to hippy\n",
      "0.8666822 caucasian is to redneck as black is to redneck\n",
      "0.8601656 caucasian is to redneck as black is to hippy\n",
      "0.85845274 caucasian is to hillbilly as black is to skinhead\n",
      "0.8545704 caucasian is to hillbilly as black is to hipster\n",
      "0.8489912 caucasian is to executive as black is to unelected\n",
      "0.84816253 caucasian is to manager as black is to manger\n",
      "==================== \n",
      "\n",
      "\n",
      "Double-Hard Debiased Analogies (0-20)\n",
      "0.9400885 caucasian is to hillbilly as black is to hillbilly\n",
      "0.939197 black is to homeless as caucasian is to homeless\n",
      "0.9372647 caucasian is to redneck as black is to redneck\n",
      "0.93210167 black is to runner as caucasian is to runner\n",
      "0.92881936 caucasian is to farmer as black is to farmer\n",
      "0.928341 black is to musician as caucasian is to musician\n",
      "0.925733 black is to slave as caucasian is to slave\n",
      "0.9253344 caucasian is to leader as black is to leader\n",
      "0.9247636 black is to criminal as caucasian is to criminal\n",
      "0.9233542 caucasian is to executive as black is to executive\n",
      "0.91936904 caucasian is to manager as black is to manager\n",
      "0.9181677 caucasian is to redneck as black is to hillbilly\n",
      "0.91514 caucasian is to hillbilly as black is to redneck\n",
      "0.8965565 caucasian is to hillbilly as black is to hick\n",
      "0.89455485 caucasian is to redneck as black is to hick\n",
      "0.8868858 black is to musician as caucasian is to novelist\n",
      "0.88101053 black is to musician as caucasian is to composer\n",
      "0.87544835 caucasian is to hillbilly as black is to hippy\n",
      "0.8753808 black is to musician as caucasian is to guitarist\n",
      "0.8748805 caucasian is to hillbilly as black is to skinhead\n"
     ]
    }
   ],
   "source": [
    "limit = 20\n",
    "print(\"Biased Analogies (0-\" + str(limit) + \")\")\n",
    "for score, analogy, _ in biasedAnalogies[:limit]:\n",
    "    print(score, analogy)\n",
    "\n",
    "print(\"=\"*20, \"\\n\\n\")\n",
    "print(\"Hard Debiased Analogies (0-\" + str(limit) + \")\")\n",
    "for score, analogy, _ in hardDebiasedAnalogies[:limit]:\n",
    "    print(score, analogy)\n",
    "\n",
    "print(\"=\"*20, \"\\n\\n\")\n",
    "print(\"Double-Hard Debiased Analogies (0-\" + str(limit) + \")\")\n",
    "for score, analogy, _ in doubleHardDebiasedAnalogies[:limit]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CE00WBZYsj3q",
    "outputId": "c298e57f-9010-46c4-af35-a2d99f826163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Evaluation\n",
      "====================\n",
      "Biased MAC: 0.8920888687350088\n",
      "HARD MAC: 0.924706514661958\n",
      "DOUBLE HARD MAC: 0.9222976140019051\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing Evaluation\")\n",
    "print(\"=\"*20,)\n",
    "\n",
    "evalTargets, evalAttrs = load_eval_terms('data/vocab/race_attributes_optm.json','role')\n",
    "\n",
    "biasedMAC, biasedDistribution = multiclass_evaluation(word_vectors, evalTargets, evalAttrs)\n",
    "print(\"Biased MAC:\", biasedMAC)\n",
    "debiasedMAC, debiasedDistribution = multiclass_evaluation(new_vocab, evalTargets, evalAttrs)\n",
    "print(\"HARD MAC:\", debiasedMAC)\n",
    "doubledebiasedMAC, doubledebiasedDistribution = multiclass_evaluation(db, evalTargets, evalAttrs)\n",
    "print(\"DOUBLE HARD MAC:\", doubledebiasedMAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Begin Utility Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPP1GXs_olTz"
   },
   "source": [
    "## Concept Categorization\n",
    "\n",
    "Cluster a set of words into different categorical subsets. Metric is fraction of the total number of the words that are correctly classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QGRDBN_sOik"
   },
   "source": [
    "### Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qil_yrAhFte0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([['acne'],\n",
       "        ['anthrax'],\n",
       "        ['arthritis'],\n",
       "        ['asthma'],\n",
       "        ['cancer'],\n",
       "        ['cholera'],\n",
       "        ['cirrhosis'],\n",
       "        ['diabetes'],\n",
       "        ['eczema'],\n",
       "        ['flu'],\n",
       "        ['glaucoma'],\n",
       "        ['hepatitis'],\n",
       "        ['leukemia'],\n",
       "        ['malnutrition'],\n",
       "        ['meningitis'],\n",
       "        ['plague'],\n",
       "        ['rheumatism'],\n",
       "        ['smallpox'],\n",
       "        ['aluminium'],\n",
       "        ['bismuth'],\n",
       "        ['cadmium'],\n",
       "        ['calcium'],\n",
       "        ['carbon'],\n",
       "        ['charcoal'],\n",
       "        ['copper'],\n",
       "        ['germanium'],\n",
       "        ['helium'],\n",
       "        ['hydrogen'],\n",
       "        ['iron'],\n",
       "        ['lithium'],\n",
       "        ['magnesium'],\n",
       "        ['neon'],\n",
       "        ['nitrogen'],\n",
       "        ['oxygen'],\n",
       "        ['platinum'],\n",
       "        ['potassium'],\n",
       "        ['silver'],\n",
       "        ['titanium'],\n",
       "        ['zinc'],\n",
       "        ['compulsion'],\n",
       "        ['conscience'],\n",
       "        ['deterrence'],\n",
       "        ['disincentive'],\n",
       "        ['dynamic'],\n",
       "        ['ethics'],\n",
       "        ['impulse'],\n",
       "        ['incentive'],\n",
       "        ['incitement'],\n",
       "        ['inducement'],\n",
       "        ['life'],\n",
       "        ['mania'],\n",
       "        ['morality'],\n",
       "        ['motivator'],\n",
       "        ['obsession'],\n",
       "        ['occasion'],\n",
       "        ['possession'],\n",
       "        ['superego'],\n",
       "        ['urge'],\n",
       "        ['wanderlust'],\n",
       "        ['cent'],\n",
       "        ['cordoba'],\n",
       "        ['dinar'],\n",
       "        ['dirham'],\n",
       "        ['dollar'],\n",
       "        ['drachma'],\n",
       "        ['escudo'],\n",
       "        ['fen'],\n",
       "        ['franc'],\n",
       "        ['guilder'],\n",
       "        ['lira'],\n",
       "        ['mark'],\n",
       "        ['penny'],\n",
       "        ['peso'],\n",
       "        ['pound'],\n",
       "        ['riel'],\n",
       "        ['rouble'],\n",
       "        ['rupee'],\n",
       "        ['shilling'],\n",
       "        ['yuan'],\n",
       "        ['zloty'],\n",
       "        ['acceptance'],\n",
       "        ['assignment'],\n",
       "        ['bill'],\n",
       "        ['bond'],\n",
       "        ['check'],\n",
       "        ['cheque'],\n",
       "        ['constitution'],\n",
       "        ['convention'],\n",
       "        ['decree'],\n",
       "        ['draft'],\n",
       "        ['floater'],\n",
       "        ['law'],\n",
       "        ['licence'],\n",
       "        ['obligation'],\n",
       "        ['opinion'],\n",
       "        ['rescript'],\n",
       "        ['sequestration'],\n",
       "        ['share'],\n",
       "        ['statute'],\n",
       "        ['straddle'],\n",
       "        ['treaty'],\n",
       "        ['concavity'],\n",
       "        ['corner'],\n",
       "        ['crinkle'],\n",
       "        ['cube'],\n",
       "        ['cuboid'],\n",
       "        ['cylinder'],\n",
       "        ['dodecahedron'],\n",
       "        ['dome'],\n",
       "        ['droop'],\n",
       "        ['fluting'],\n",
       "        ['icosahedron'],\n",
       "        ['indentation'],\n",
       "        ['jag'],\n",
       "        ['knob'],\n",
       "        ['octahedron'],\n",
       "        ['ovoid'],\n",
       "        ['ring'],\n",
       "        ['salient'],\n",
       "        ['taper'],\n",
       "        ['tetrahedron'],\n",
       "        ['ball'],\n",
       "        ['celebration'],\n",
       "        ['ceremony'],\n",
       "        ['commemoration'],\n",
       "        ['commencement'],\n",
       "        ['coronation'],\n",
       "        ['dance'],\n",
       "        ['enthronement'],\n",
       "        ['feast'],\n",
       "        ['fete'],\n",
       "        ['fiesta'],\n",
       "        ['fundraiser'],\n",
       "        ['funeral'],\n",
       "        ['graduation'],\n",
       "        ['inaugural'],\n",
       "        ['pageantry'],\n",
       "        ['party'],\n",
       "        ['prom'],\n",
       "        ['rededication'],\n",
       "        ['wedding'],\n",
       "        ['baccarat'],\n",
       "        ['basketball'],\n",
       "        ['beano'],\n",
       "        ['bowling'],\n",
       "        ['chess'],\n",
       "        ['curling'],\n",
       "        ['faro'],\n",
       "        ['football'],\n",
       "        ['golf'],\n",
       "        ['handball'],\n",
       "        ['keno'],\n",
       "        ['lotto'],\n",
       "        ['nap'],\n",
       "        ['raffle'],\n",
       "        ['rugby'],\n",
       "        ['soccer'],\n",
       "        ['softball'],\n",
       "        ['tennis'],\n",
       "        ['volleyball'],\n",
       "        ['whist'],\n",
       "        ['aeon'],\n",
       "        ['date'],\n",
       "        ['day'],\n",
       "        ['epoch'],\n",
       "        ['future'],\n",
       "        ['gestation'],\n",
       "        ['hereafter'],\n",
       "        ['menopause'],\n",
       "        ['moment'],\n",
       "        ['nonce'],\n",
       "        ['period'],\n",
       "        ['quaternary'],\n",
       "        ['today'],\n",
       "        ['tomorrow'],\n",
       "        ['tonight'],\n",
       "        ['yesterday'],\n",
       "        ['yesteryear'],\n",
       "        ['anger'],\n",
       "        ['desire'],\n",
       "        ['fear'],\n",
       "        ['happiness'],\n",
       "        ['joy'],\n",
       "        ['love'],\n",
       "        ['pain'],\n",
       "        ['passion'],\n",
       "        ['pleasure'],\n",
       "        ['sadness'],\n",
       "        ['sensitivity'],\n",
       "        ['shame'],\n",
       "        ['wonder'],\n",
       "        ['ache'],\n",
       "        ['backache'],\n",
       "        ['bellyache'],\n",
       "        ['burn'],\n",
       "        ['earache'],\n",
       "        ['headache'],\n",
       "        ['lumbago'],\n",
       "        ['migraine'],\n",
       "        ['neuralgia'],\n",
       "        ['sciatica'],\n",
       "        ['soreness'],\n",
       "        ['sting'],\n",
       "        ['stinging'],\n",
       "        ['stitch'],\n",
       "        ['suffering'],\n",
       "        ['tenderness'],\n",
       "        ['throb'],\n",
       "        ['toothache'],\n",
       "        ['torment'],\n",
       "        ['architect'],\n",
       "        ['artist'],\n",
       "        ['builder'],\n",
       "        ['constructor'],\n",
       "        ['craftsman'],\n",
       "        ['designer'],\n",
       "        ['developer'],\n",
       "        ['farmer'],\n",
       "        ['inventor'],\n",
       "        ['maker'],\n",
       "        ['manufacturer'],\n",
       "        ['musician'],\n",
       "        ['originator'],\n",
       "        ['painter'],\n",
       "        ['photographer'],\n",
       "        ['producer'],\n",
       "        ['tailor'],\n",
       "        ['agency'],\n",
       "        ['branch'],\n",
       "        ['brigade'],\n",
       "        ['bureau'],\n",
       "        ['club'],\n",
       "        ['committee'],\n",
       "        ['company'],\n",
       "        ['confederacy'],\n",
       "        ['department'],\n",
       "        ['divan'],\n",
       "        ['family'],\n",
       "        ['house'],\n",
       "        ['household'],\n",
       "        ['league'],\n",
       "        ['legion'],\n",
       "        ['nation'],\n",
       "        ['office'],\n",
       "        ['platoon'],\n",
       "        ['team'],\n",
       "        ['tribe'],\n",
       "        ['troop'],\n",
       "        ['aircraft'],\n",
       "        ['airplane'],\n",
       "        ['automobile'],\n",
       "        ['bicycle'],\n",
       "        ['boat'],\n",
       "        ['car'],\n",
       "        ['cruiser'],\n",
       "        ['helicopter'],\n",
       "        ['motorcycle'],\n",
       "        ['pickup'],\n",
       "        ['rocket'],\n",
       "        ['ship'],\n",
       "        ['truck'],\n",
       "        ['van'],\n",
       "        ['airstream'],\n",
       "        ['aurora'],\n",
       "        ['blast'],\n",
       "        ['clemency'],\n",
       "        ['cloud'],\n",
       "        ['cloudburst'],\n",
       "        ['crosswind'],\n",
       "        ['cyclone'],\n",
       "        ['drizzle'],\n",
       "        ['fog'],\n",
       "        ['hurricane'],\n",
       "        ['lightning'],\n",
       "        ['rainstorm'],\n",
       "        ['sandstorm'],\n",
       "        ['shower'],\n",
       "        ['snowfall'],\n",
       "        ['thunderstorm'],\n",
       "        ['tornado'],\n",
       "        ['twister'],\n",
       "        ['typhoon'],\n",
       "        ['wind'],\n",
       "        ['bear'],\n",
       "        ['bull'],\n",
       "        ['camel'],\n",
       "        ['cat'],\n",
       "        ['cow'],\n",
       "        ['deer'],\n",
       "        ['dog'],\n",
       "        ['elephant'],\n",
       "        ['horse'],\n",
       "        ['kitten'],\n",
       "        ['lion'],\n",
       "        ['monkey'],\n",
       "        ['mouse'],\n",
       "        ['oyster'],\n",
       "        ['puppy'],\n",
       "        ['rat'],\n",
       "        ['sheep'],\n",
       "        ['tiger'],\n",
       "        ['turtle'],\n",
       "        ['zebra'],\n",
       "        ['allocation'],\n",
       "        ['allotment'],\n",
       "        ['capital'],\n",
       "        ['credit'],\n",
       "        ['dispensation'],\n",
       "        ['fund'],\n",
       "        ['gain'],\n",
       "        ['gold'],\n",
       "        ['hoard'],\n",
       "        ['income'],\n",
       "        ['interest'],\n",
       "        ['investment'],\n",
       "        ['margin'],\n",
       "        ['mortgage'],\n",
       "        ['payoff'],\n",
       "        ['profit'],\n",
       "        ['quota'],\n",
       "        ['taxation'],\n",
       "        ['trove'],\n",
       "        ['venture'],\n",
       "        ['wager'],\n",
       "        ['apple'],\n",
       "        ['banana'],\n",
       "        ['berry'],\n",
       "        ['cherry'],\n",
       "        ['grape'],\n",
       "        ['kiwi'],\n",
       "        ['lemon'],\n",
       "        ['mango'],\n",
       "        ['melon'],\n",
       "        ['olive'],\n",
       "        ['orange'],\n",
       "        ['peach'],\n",
       "        ['pear'],\n",
       "        ['pineapple'],\n",
       "        ['strawberry'],\n",
       "        ['watermelon'],\n",
       "        ['anchorage'],\n",
       "        ['borderland'],\n",
       "        ['borough'],\n",
       "        ['caliphate'],\n",
       "        ['canton'],\n",
       "        ['city'],\n",
       "        ['country'],\n",
       "        ['county'],\n",
       "        ['kingdom'],\n",
       "        ['land'],\n",
       "        ['metropolis'],\n",
       "        ['parish'],\n",
       "        ['prefecture'],\n",
       "        ['riverside'],\n",
       "        ['seafront'],\n",
       "        ['shire'],\n",
       "        ['state'],\n",
       "        ['suburb'],\n",
       "        ['sultanate'],\n",
       "        ['town'],\n",
       "        ['village'],\n",
       "        ['acacia'],\n",
       "        ['casuarina'],\n",
       "        ['chestnut'],\n",
       "        ['cinchona'],\n",
       "        ['coco'],\n",
       "        ['conifer'],\n",
       "        ['fig'],\n",
       "        ['hornbeam'],\n",
       "        ['jacaranda'],\n",
       "        ['lime'],\n",
       "        ['mandarin'],\n",
       "        ['mangrove'],\n",
       "        ['oak'],\n",
       "        ['palm'],\n",
       "        ['pine'],\n",
       "        ['pistachio'],\n",
       "        ['rowan'],\n",
       "        ['samba'],\n",
       "        ['sapling'],\n",
       "        ['sycamore'],\n",
       "        ['walnut'],\n",
       "        ['chill'],\n",
       "        ['coolness'],\n",
       "        ['deflection'],\n",
       "        ['diameter'],\n",
       "        ['extension'],\n",
       "        ['glow'],\n",
       "        ['heaviness'],\n",
       "        ['length'],\n",
       "        ['mass'],\n",
       "        ['momentum'],\n",
       "        ['plasticity'],\n",
       "        ['poundage'],\n",
       "        ['radius'],\n",
       "        ['reflexion'],\n",
       "        ['shortness'],\n",
       "        ['snap'],\n",
       "        ['stretch'],\n",
       "        ['temperature'],\n",
       "        ['visibility'],\n",
       "        ['weight']], dtype=object),\n",
       " 'y': array(['illness', 'illness', 'illness', 'illness', 'illness', 'illness',\n",
       "        'illness', 'illness', 'illness', 'illness', 'illness', 'illness',\n",
       "        'illness', 'illness', 'illness', 'illness', 'illness', 'illness',\n",
       "        'chemical_element', 'chemical_element', 'chemical_element',\n",
       "        'chemical_element', 'chemical_element', 'chemical_element',\n",
       "        'chemical_element', 'chemical_element', 'chemical_element',\n",
       "        'chemical_element', 'chemical_element', 'chemical_element',\n",
       "        'chemical_element', 'chemical_element', 'chemical_element',\n",
       "        'chemical_element', 'chemical_element', 'chemical_element',\n",
       "        'chemical_element', 'chemical_element', 'chemical_element',\n",
       "        'motivation', 'motivation', 'motivation', 'motivation',\n",
       "        'motivation', 'motivation', 'motivation', 'motivation',\n",
       "        'motivation', 'motivation', 'motivation', 'motivation',\n",
       "        'motivation', 'motivation', 'motivation', 'motivation',\n",
       "        'motivation', 'motivation', 'motivation', 'motivation',\n",
       "        'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "        'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "        'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "        'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "        'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "        'monetary_unit', 'legal_document', 'legal_document',\n",
       "        'legal_document', 'legal_document', 'legal_document',\n",
       "        'legal_document', 'legal_document', 'legal_document',\n",
       "        'legal_document', 'legal_document', 'legal_document',\n",
       "        'legal_document', 'legal_document', 'legal_document',\n",
       "        'legal_document', 'legal_document', 'legal_document',\n",
       "        'legal_document', 'legal_document', 'legal_document',\n",
       "        'legal_document', 'solid', 'solid', 'solid', 'solid', 'solid',\n",
       "        'solid', 'solid', 'solid', 'solid', 'solid', 'solid', 'solid',\n",
       "        'solid', 'solid', 'solid', 'solid', 'solid', 'solid', 'solid',\n",
       "        'solid', 'social_occasion', 'social_occasion', 'social_occasion',\n",
       "        'social_occasion', 'social_occasion', 'social_occasion',\n",
       "        'social_occasion', 'social_occasion', 'social_occasion',\n",
       "        'social_occasion', 'social_occasion', 'social_occasion',\n",
       "        'social_occasion', 'social_occasion', 'social_occasion',\n",
       "        'social_occasion', 'social_occasion', 'social_occasion',\n",
       "        'social_occasion', 'social_occasion', 'game', 'game', 'game',\n",
       "        'game', 'game', 'game', 'game', 'game', 'game', 'game', 'game',\n",
       "        'game', 'game', 'game', 'game', 'game', 'game', 'game', 'game',\n",
       "        'game', 'time', 'time', 'time', 'time', 'time', 'time', 'time',\n",
       "        'time', 'time', 'time', 'time', 'time', 'time', 'time', 'time',\n",
       "        'time', 'time', 'feeling', 'feeling', 'feeling', 'feeling',\n",
       "        'feeling', 'feeling', 'feeling', 'feeling', 'feeling', 'feeling',\n",
       "        'feeling', 'feeling', 'feeling', 'pain', 'pain', 'pain', 'pain',\n",
       "        'pain', 'pain', 'pain', 'pain', 'pain', 'pain', 'pain', 'pain',\n",
       "        'pain', 'pain', 'pain', 'pain', 'pain', 'pain', 'pain', 'creator',\n",
       "        'creator', 'creator', 'creator', 'creator', 'creator', 'creator',\n",
       "        'creator', 'creator', 'creator', 'creator', 'creator', 'creator',\n",
       "        'creator', 'creator', 'creator', 'creator', 'social_unit',\n",
       "        'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "        'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "        'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "        'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "        'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "        'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle',\n",
       "        'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle',\n",
       "        'vehicle', 'vehicle', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "        'atmospheric_phenomenon', 'atmospheric_phenomenon', 'animal',\n",
       "        'animal', 'animal', 'animal', 'animal', 'animal', 'animal',\n",
       "        'animal', 'animal', 'animal', 'animal', 'animal', 'animal',\n",
       "        'animal', 'animal', 'animal', 'animal', 'animal', 'animal',\n",
       "        'animal', 'assets', 'assets', 'assets', 'assets', 'assets',\n",
       "        'assets', 'assets', 'assets', 'assets', 'assets', 'assets',\n",
       "        'assets', 'assets', 'assets', 'assets', 'assets', 'assets',\n",
       "        'assets', 'assets', 'assets', 'assets', 'edible_fruit',\n",
       "        'edible_fruit', 'edible_fruit', 'edible_fruit', 'edible_fruit',\n",
       "        'edible_fruit', 'edible_fruit', 'edible_fruit', 'edible_fruit',\n",
       "        'edible_fruit', 'edible_fruit', 'edible_fruit', 'edible_fruit',\n",
       "        'edible_fruit', 'edible_fruit', 'edible_fruit', 'district',\n",
       "        'district', 'district', 'district', 'district', 'district',\n",
       "        'district', 'district', 'district', 'district', 'district',\n",
       "        'district', 'district', 'district', 'district', 'district',\n",
       "        'district', 'district', 'district', 'district', 'district', 'tree',\n",
       "        'tree', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree',\n",
       "        'tree', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree',\n",
       "        'tree', 'tree', 'tree', 'tree', 'physical_property',\n",
       "        'physical_property', 'physical_property', 'physical_property',\n",
       "        'physical_property', 'physical_property', 'physical_property',\n",
       "        'physical_property', 'physical_property', 'physical_property',\n",
       "        'physical_property', 'physical_property', 'physical_property',\n",
       "        'physical_property', 'physical_property', 'physical_property',\n",
       "        'physical_property', 'physical_property', 'physical_property',\n",
       "        'physical_property'], dtype=object)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_names = [\"EN-AP\", \"EN-ESSLI-2c\", \"EN-ESSLI-2b\", \"EN-ESSLI-1a\", \"EN-BATTIG\", \"EN-BLESS\", ]\n",
    "bench_paths = [os.path.join('data', 'concept_clustering', name) for name in bench_names]\n",
    "benchmarks = create_bunches(bench_paths)\n",
    "\n",
    "# show benchmark example to confirm\n",
    "benchmarks['EN-AP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cjInIq5Nx88",
    "outputId": "3ad3f6f7-4995-43ae-d4f4-0de707deb772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5091463414634146\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5555555555555556\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.6590909090909091\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.3375944218477629\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.5181347150259068\n",
      "\n",
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5914634146341463\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5555555555555556\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.7045454545454546\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.4186519465427077\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.5854922279792746\n",
      "\n",
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5487804878048781\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5777777777777778\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.75\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.4264962231260895\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.5854922279792746\n"
     ]
    }
   ],
   "source": [
    "evaluate_cate(word_vectors, benchmarks)\n",
    "print()\n",
    "evaluate_cate(new_vocab, benchmarks)\n",
    "print()\n",
    "evaluate_cate(db, benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oykWwAIQDGd7"
   },
   "source": [
    "Control:\n",
    "\n",
    "Embeddings given to us by the Black is to Criminal Paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMCsQT9iP3Jn",
    "outputId": "49f57c85-8a90-4a5e-c2c2-4795d4e7fa2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5091463414634146\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5555555555555556\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.7045454545454546\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.34166182452062754\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.538860103626943\n",
      "\n",
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5945121951219512\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5333333333333333\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.875\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.6818181818181819\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.41545613015688554\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.6269430051813472\n"
     ]
    }
   ],
   "source": [
    "biased_path = os.path.join('data', 'data_vocab_race_pre_trained.w2v')\n",
    "biased_embeddings, embedding_dim = load_legacy_w2v(biased_path, 50)\n",
    "# take out words that do not only contain letters\n",
    "biased_word_vectors = pruneWordVecs(biased_embeddings)\n",
    "evaluate_cate(biased_word_vectors, benchmarks)\n",
    "\n",
    "print()\n",
    "\n",
    "debiased_path = os.path.join('data','data_vocab_race_hard_debias.w2v')\n",
    "debiased_embeddings, embedding_dim = load_legacy_w2v(debiased_path, 50)\n",
    "# take out words that do not only contain letters\n",
    "debiased_word_vectors = pruneWordVecs(debiased_embeddings)\n",
    "evaluate_cate(debiased_word_vectors, benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YelvhkXwR6tP"
   },
   "source": [
    "Conclusion:\n",
    "\n",
    "Utility is preserved\n",
    "*   AP - goes up\n",
    "*   ESSLI-2c - stays the same\n",
    "*   ESSLI-2b - stays the same\n",
    "*   ESSLI-1a - goes up then back down\n",
    "*   BATTIG - goes up (sometimes goes back down)\n",
    "*   BLESS - goes up (sometimes goes back down)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D00ZeRmvW5v"
   },
   "source": [
    "## Analogy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DyP17ZSg0NyT"
   },
   "outputs": [],
   "source": [
    "# Create text file for each category.\n",
    "with open(os.path.join('data', 'analogies', 'EN-GOOGLE', 'EN-GOOGLE.txt'), \"r\") as file:\n",
    "        lines = file.read().splitlines()\n",
    "\n",
    "        questions = []\n",
    "        answers = []\n",
    "        category = []\n",
    "        dummy = os.path.join('data', 'analogies', 'dummy.txt')\n",
    "\n",
    "        f = open(dummy, \"w\")\n",
    "        f.write(\"/n\")\n",
    "        for line in lines:\n",
    "            if line.startswith(\":\"):\n",
    "                \n",
    "                # delete last \"/n\" and close file\n",
    "                f.seek(f.tell()-1)\n",
    "                f.truncate()\n",
    "                f.close()\n",
    "\n",
    "                # start new file for new category\n",
    "                cat = line.lower().split()[1]\n",
    "                f = open(os.path.join('data', 'analogies', f'{cat}.txt'), \"w\")\n",
    "\n",
    "            else:\n",
    "                f.write(line+'\\n')\n",
    "\n",
    "        os.remove(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRiELeLn6CnS",
    "outputId": "6ddae7d5-4e25-4677-d523-fffb292553a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Biased ----------\n",
      "family.txt:\n",
      "ACCURACY TOP1: 70.26% (267/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 8.06% (75/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 17.86% (135/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 68.62% (914/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 40.91% (432/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 64.68% (683/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 48.85% (762/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 33.65% (424/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.26% (319/567)\n",
      "Questions seen/total: 46.24% (8897/19241)\n",
      "Semantic accuracy: 70.26%  (267/380)\n",
      "Syntactic accuracy: 43.96%  (3744/8517)\n",
      "Total accuracy: 45.08%  (4011/8897)\n",
      "\n",
      "---------- Hard Biased ----------\n",
      "family.txt:\n",
      "ACCURACY TOP1: 70.26% (267/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 8.06% (75/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 17.86% (135/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 68.62% (914/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 40.91% (432/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 64.68% (683/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 48.85% (762/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 33.65% (424/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.26% (319/567)\n",
      "Questions seen/total: 46.24% (8897/19241)\n",
      "Semantic accuracy: 70.26%  (267/380)\n",
      "Syntactic accuracy: 43.96%  (3744/8517)\n",
      "Total accuracy: 45.08%  (4011/8897)\n",
      "\n",
      "---------- Double Hard Biased ----------\n",
      "family.txt:\n",
      "ACCURACY TOP1: 71.32% (271/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 7.74% (72/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 16.27% (123/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 58.18% (775/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 41.00% (433/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 61.84% (653/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 45.64% (712/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 32.78% (413/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.61% (321/567)\n",
      "Questions seen/total: 46.24% (8897/19241)\n",
      "Semantic accuracy: 71.32%  (271/380)\n",
      "Syntactic accuracy: 41.12%  (3502/8517)\n",
      "Total accuracy: 42.41%  (3773/8897)\n"
     ]
    }
   ],
   "source": [
    "print('-'*10, 'Biased', '-'*10)\n",
    "evaluate_analogies(word_vectors)\n",
    "print()\n",
    "print('-'*10, 'Hard Biased', '-'*10)\n",
    "evaluate_analogies(new_vocab)\n",
    "print()\n",
    "print('-'*10, 'Double Hard Biased', '-'*10)\n",
    "evaluate_analogies(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujnFa05D90rb"
   },
   "source": [
    "Control:\n",
    "\n",
    "Embeddings given to us by the Black is to Criminal Paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qt4q_Hs09qGQ",
    "outputId": "cfbe20c8-a535-4c46-ac12-a6fff4b3024c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family.txt:\n",
      "ACCURACY TOP1: 70.26% (267/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 8.06% (75/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 17.86% (135/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 68.62% (914/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 40.91% (432/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 64.68% (683/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 48.85% (762/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 33.65% (424/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.26% (319/567)\n",
      "Questions seen/total: 46.24% (8897/19241)\n",
      "Semantic accuracy: 70.26%  (267/380)\n",
      "Syntactic accuracy: 43.96%  (3744/8517)\n",
      "Total accuracy: 45.08%  (4011/8897)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 72.37% (275/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 8.92% (83/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 19.31% (146/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 68.17% (908/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 40.91% (432/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 63.07% (666/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 50.19% (783/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 34.21% (431/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.26% (319/567)\n",
      "Questions seen/total: 46.24% (8897/19241)\n",
      "Semantic accuracy: 72.37%  (275/380)\n",
      "Syntactic accuracy: 44.24%  (3768/8517)\n",
      "Total accuracy: 45.44%  (4043/8897)\n"
     ]
    }
   ],
   "source": [
    "evaluate_analogies(biased_word_vectors)\n",
    "evaluate_analogies(debiased_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE1-1Faf016M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Experiments Utility.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
