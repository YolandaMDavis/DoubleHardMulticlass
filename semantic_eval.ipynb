{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKtlro5Wv--0"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Gea6Cv0OwOA6"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__  # I use '3.6.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z1S5Yk0J-TvW"
   },
   "outputs": [],
   "source": [
    "def load_legacy_w2v(w2v_file, dim=50):\n",
    "    vectors = {}\n",
    "    with open(w2v_file, 'r') as f:\n",
    "        for line in f:\n",
    "            vect = line.strip().rsplit()\n",
    "            word = vect[0]\n",
    "            vect = np.array([float(x) for x in vect[1:]])\n",
    "            if(dim == len(vect)):\n",
    "                vectors[word] = vect\n",
    "        \n",
    "    return vectors, dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn_LWQry5E30"
   },
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wMcfhTY472_x"
   },
   "outputs": [],
   "source": [
    "# Pretrained embeddings from the l2 reddit corpus\n",
    "# https://github.com/TManzini/DebiasMulticlassWordEmbedding\n",
    "reddit_path = os.path.join('data', 'data_vocab_race_pre_trained.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx5GJ4xN68qO"
   },
   "source": [
    "# Debias\n",
    "\n",
    "Two steps:\n",
    "1. Identify the bias subspace\n",
    "2. remove this component from the set of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftFeABoF_vsS",
    "outputId": "14ed4f40-7296-4cae-936c-1957d6ac9404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining words after prune: 44895\n"
     ]
    }
   ],
   "source": [
    "def pruneWordVecs(wordVecs):\n",
    "    newWordVecs = {}\n",
    "    for word, vec in wordVecs.items():\n",
    "        valid=True\n",
    "        if(not all([c.isalpha() for c in word])):\n",
    "            valid = False\n",
    "        if(valid):\n",
    "            newWordVecs[word] = vec\n",
    "    return newWordVecs\n",
    "\n",
    "\n",
    "# Load unbiased embeddings\n",
    "reddit_embeddings, embedding_dim = load_legacy_w2v(reddit_path, 50)\n",
    "\n",
    "# take out words that do not only contain letters\n",
    "word_vectors = pruneWordVecs(reddit_embeddings)\n",
    "print(\"Remaining words after prune:\", len(word_vectors))\n",
    "\n",
    "# Get defining sets\n",
    "def_sets_path = os.path.join('data', 'vocab', 'race_attributes_optm.json')\n",
    "with open(def_sets_path, 'r') as f:\n",
    "    def_sets =json.load(f)\n",
    "def_sets = {i: v for i, v in enumerate(def_sets[\"definite_sets\"])}\n",
    "\n",
    "# Calculate the mean for each defining set\n",
    "means = {}\n",
    "for k, v in def_sets.items():\n",
    "    wSet = []\n",
    "    for word in v:\n",
    "        wSet.append(word_vectors[word])\n",
    "    set_vectors = np.array(wSet)\n",
    "    means[k] = np.mean(set_vectors, axis=0)\n",
    "\n",
    "# Subtract each word by the mean of the set\n",
    "matrix = []\n",
    "for k, v in def_sets.items():\n",
    "    wSet = []\n",
    "    for word in v:\n",
    "        wSet.append(word_vectors[word])\n",
    "    set_vectors = np.array(wSet)\n",
    "    diffs = set_vectors - means[k]\n",
    "    matrix.append(diffs)\n",
    "matrix = np.concatenate(matrix)\n",
    "\n",
    "# PCA\n",
    "k = 2\n",
    "pca = PCA(n_components=k)\n",
    "pca.fit(matrix)\n",
    "subspace = pca.components_\n",
    "\n",
    "# ---------------  Remove the bias subspace -------------------\n",
    "\n",
    "# Get neutral words\n",
    "attribute_path = os.path.join('data', 'vocab', 'race_attributes_optm.json')\n",
    "with open(attribute_path, \"r\") as f:\t\n",
    "    attributes = json.load(f)\n",
    "    roles = attributes[\"analogy_templates\"]['role']\n",
    "\n",
    "neutral_words = []\n",
    "for value in roles.values():\n",
    "    neutral_words.extend(value)\n",
    "\n",
    "new_vocab = word_vectors.copy()\n",
    "\n",
    "# Neutralize\n",
    "for word in neutral_words:\n",
    "    if word in word_vectors:\n",
    "        embedding = word_vectors[word]\n",
    "\n",
    "        # Project neutral word onto bias subspace\n",
    "        v_b = np.zeros_like(embedding)\n",
    "        for component in subspace:\n",
    "            v_b += np.dot(embedding.transpose(), component) * component\n",
    "\n",
    "        # Remove bias subspace and normalize to unit vector\n",
    "        new_v = (embedding - v_b) / np.linalg.norm(embedding - v_b)\n",
    "        new_vocab[word] = new_v\n",
    "\n",
    "    # Normalize ALL words, not just the neurtral words chosen\n",
    "    for word, embedding in new_vocab.items():\n",
    "        new_vocab[word] = embedding / np.linalg.norm(embedding)\n",
    "\n",
    "# Equalize\n",
    "for eq_set in def_sets.values():\n",
    "    mean = np.zeros(matrix.shape[1])\n",
    "\n",
    "    for word in eq_set:\n",
    "        # Calculate the mean of the words in the set\n",
    "        mean += new_vocab[word]\n",
    "        mean /= float(len(eq_set))\n",
    "\n",
    "        # Project this mean onto the bias subspace\n",
    "        mean_b = np.zeros_like(mean)\n",
    "        for component in subspace:\n",
    "            mean_b += np.dot(mean.transpose(), component) * component\n",
    "\n",
    "        # Remove the bias subspace from the mean\n",
    "        upsilon = mean - mean_b\n",
    "\n",
    "    for word in eq_set:\n",
    "        embedding = new_vocab[word]\n",
    "\n",
    "        # project the word from the equality set onto the bias subspace\n",
    "        v_b = np.zeros_like(embedding)\n",
    "        for component in subspace:\n",
    "            v_b += np.dot(embedding.transpose(), component) * component\n",
    "\n",
    "        # centering? Subtract the word projected onto the bia subspace by the mean projected onto the bias subspace\n",
    "        frac = (v_b - mean_b) / np.linalg.norm(v_b - mean_b)\n",
    "\n",
    "        # why? equalize?\n",
    "        new_v = upsilon + np.sqrt(1 - np.sum(np.square(upsilon))) * frac\n",
    "\n",
    "        # Update with the new embedding\n",
    "        new_vocab[word] = new_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6KFlhjFKkAn"
   },
   "source": [
    "# Double Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0jwLEn0o2T9b"
   },
   "outputs": [],
   "source": [
    "# Double Hard\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "\n",
    "def calculate_main_pca_components(word_vectors):\n",
    "    \"\"\"From https://github.com/uvavision/Double-Hard-Debias/blob/master/GloVe_Debias.ipynb\"\"\"\n",
    "    vectors = word_vectors.values() # word_vectors.vectors\n",
    "    wv_mean = np.mean(np.array(vectors), axis=0)\n",
    "    wv_hat = vectors - wv_mean\n",
    "    main_pca = PCA()\n",
    "    main_pca.fit(wv_hat)\n",
    "    return main_pca\n",
    "\n",
    "def neutralize_and_equalize_with_frequency_removal(vocab, words, eq_sets, bias_subspace, embedding_dim, principal_component):\n",
    "    \"\"\"\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    words - words to neutralize\n",
    "    eq_sets - set of equality sets\n",
    "    bias_subspace - subspace of bias from identify_bias_subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    if bias_subspace.ndim == 1:\n",
    "        bias_subspace = np.expand_dims(bias_subspace, 0)\n",
    "    elif bias_subspace.ndim != 2:\n",
    "        raise ValueError(\"bias subspace should be either a matrix or vector\")\n",
    "\n",
    "    full_set = set(list(words) + [word for eq_words in eq_sets for word in eq_words])\n",
    "    freq_vocab = vocab.copy()\n",
    "\n",
    "    for word in full_set:\n",
    "        vector = freq_vocab[word]\n",
    "        projection = np.dot(np.dot(np.transpose(principal_component), vector), principal_component)\n",
    "        freq_vocab[word] = vector - projection\n",
    "\n",
    "    return neutralize_and_equalize(freq_vocab, words, eq_sets, bias_subspace, embedding_dim)\n",
    "\n",
    "\n",
    "def neutralize_and_equalize(vocab, words, eq_sets, bias_subspace, embedding_dim):\n",
    "    \"\"\"\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    words - words to neutralize\n",
    "    eq_sets - set of equality sets\n",
    "    bias_subspace - subspace of bias from identify_bias_subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    if bias_subspace.ndim == 1:\n",
    "        bias_subspace = np.expand_dims(bias_subspace, 0)\n",
    "    elif bias_subspace.ndim != 2:\n",
    "        raise ValueError(\"bias subspace should be either a matrix or vector\")\n",
    "\n",
    "    new_vocab = vocab.copy()\n",
    "    for w in words:\n",
    "        # get projection onto bias subspace\n",
    "        if w in vocab:\n",
    "            v = vocab[w]\n",
    "            v_b = project_onto_subspace(v, bias_subspace)\n",
    "\n",
    "            new_v = (v - v_b) / np.linalg.norm(v - v_b)\n",
    "            #print np.linalg.norm(new_v)\n",
    "            # update embedding\n",
    "            new_vocab[w] = new_v\n",
    "\n",
    "    normalize(new_vocab)\n",
    "\n",
    "    for eq_set in eq_sets:\n",
    "        mean = np.zeros((embedding_dim,))\n",
    "\n",
    "        #Make sure the elements in the eq sets are valid\n",
    "        cleanEqSet = []\n",
    "        for w in eq_set:\n",
    "            try:\n",
    "                _ = new_vocab[w]\n",
    "                cleanEqSet.append(w)\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "        for w in cleanEqSet:\n",
    "            mean += new_vocab[w]\n",
    "        mean /= float(len(cleanEqSet))\n",
    "\n",
    "        mean_b = project_onto_subspace(mean, bias_subspace)\n",
    "        upsilon = mean - mean_b\n",
    "\n",
    "        for w in cleanEqSet:\n",
    "            v = new_vocab[w]\n",
    "            v_b = project_onto_subspace(v, bias_subspace)\n",
    "\n",
    "            frac = (v_b - mean_b) / np.linalg.norm(v_b - mean_b)\n",
    "            new_v = upsilon + np.sqrt(1 - np.sum(np.square(upsilon))) * frac\n",
    "\n",
    "            new_vocab[w] = new_v\n",
    "\n",
    "    return new_vocab\n",
    "\n",
    "def project_onto_subspace(vector, subspace):\n",
    "    v_b = np.zeros_like(vector)\n",
    "    for component in subspace:\n",
    "        v_b += np.dot(vector.transpose(), component) * component\n",
    "    return v_b\n",
    "\n",
    "def normalize(word_vectors):\n",
    "    for k, v in word_vectors.items():\n",
    "        word_vectors[k] = v / np.linalg.norm(v)\n",
    "\n",
    "def cluster(X1, random_state, y_true, num=2):\n",
    "    kmeans_1 = KMeans(n_clusters=num, random_state=random_state).fit(X1)\n",
    "    y_pred_1 = kmeans_1.predict(X1)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_1) ]\n",
    "    return max(sum(correct)/float(len(correct)), 1 - sum(correct)/float(len(correct)))\n",
    "\n",
    "def get_most_biased(word_vectors, subspace, n_biased=500):\n",
    "    \"\"\"\n",
    "    Get vectors with most positive and negative bias wrt subspace.\n",
    "    \"\"\"\n",
    "\n",
    "    biases = {}\n",
    "\n",
    "    for word, vector in word_vectors.items():\n",
    "        # 1d case\n",
    "        bias = np.dot(vector, subspace)\n",
    "        biases[word] = bias\n",
    "\n",
    "    sorted_biases = sorted(list(biases.items()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    positive_bias = sorted_biases[:n_biased]\n",
    "    negative_bias = list(reversed(sorted_biases[-n_biased:]))\n",
    "\n",
    "    return positive_bias, negative_bias\n",
    "\n",
    "\n",
    "def identify_bias_subspace(vocab, def_sets, subspace_dim, embedding_dim):\n",
    "    \"\"\"\n",
    "    Similar to bolukbasi's implementation at\n",
    "    https://github.com/tolga-b/debiaswe/blob/master/debiaswe/debias.py\n",
    "    vocab - dictionary mapping words to embeddings\n",
    "    def_sets - sets of words that represent extremes? of the subspace\n",
    "            we're interested in (e.g. man-woman, boy-girl, etc. for binary gender)\n",
    "    subspace_dim - number of vectors defining the subspace\n",
    "    embedding_dim - dimensions of the word embeddings\n",
    "    \"\"\"\n",
    "    # calculate means of defining sets\n",
    "    means = {}\n",
    "    for k, v in def_sets.items():\n",
    "        wSet = []\n",
    "        for w in v:\n",
    "            try:\n",
    "                wSet.append(vocab[w])\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        set_vectors = np.array(wSet)\n",
    "        means[k] = np.mean(set_vectors, axis=0)\n",
    "\n",
    "    # calculate vectors to perform PCA\n",
    "    matrix = []\n",
    "    for k, v in def_sets.items():\n",
    "        wSet = []\n",
    "        for w in v:\n",
    "            try:\n",
    "                wSet.append(vocab[w])\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        set_vectors = np.array(wSet)\n",
    "        diffs = set_vectors - means[k]\n",
    "        matrix.append(diffs)\n",
    "\n",
    "    matrix = np.concatenate(matrix)\n",
    "\n",
    "    pca = PCA(n_components=subspace_dim)\n",
    "    pca.fit(matrix)\n",
    "\n",
    "    return pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WkDVK8oxLVWb"
   },
   "outputs": [],
   "source": [
    "subspace = identify_bias_subspace(word_vectors, def_sets, 2, 50)\n",
    "\n",
    "# get all positive and negative words words within the subspace (moving towards or away from some race)\n",
    "positive_words, negative_words = get_most_biased(word_vectors, subspace[0], 1000)\n",
    "\n",
    "# get all biased words (minus those in the defining/equality set)\n",
    "positive_words = [word for word, sim in positive_words]\n",
    "negative_words = [word for word, sim in negative_words]\n",
    "\n",
    "eq_sets = [set for _, set in def_sets.items()]\n",
    "biased_words = set(positive_words + negative_words) - set(np.array(eq_sets).flatten())\n",
    "\n",
    "# 1 for more positive biased words, 0 for more negative biased words\n",
    "y_true = [ 1 if word in positive_words else 0 for word in biased_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ls4d2Z9eGYZM"
   },
   "outputs": [],
   "source": [
    "# calculate decentralized embedding pca\n",
    "summation = np.zeros(shape=50)\n",
    "for word in word_vectors:\n",
    "    summation += word_vectors[word]\n",
    "wv_mean = summation/len(word_vectors)\n",
    "\n",
    "wv_hat = []\n",
    "for word in word_vectors:\n",
    "    wv_hat.append(word_vectors[word] - wv_mean)\n",
    "\n",
    "main_pca = PCA()\n",
    "main_pca.fit(wv_hat)\n",
    "main_pca = main_pca.components_\n",
    "\n",
    "# run debias for each principal component and capture best precision\n",
    "precisions = []\n",
    "debiases = []\n",
    "\n",
    "for pc in main_pca:\n",
    "    debiased_frequency = neutralize_and_equalize_with_frequency_removal(word_vectors, biased_words, eq_sets, subspace, 50, pc)\n",
    "    x_dhd = [debiased_frequency[word] for word in biased_words]    \n",
    "    precisions.append(cluster(X1=np.array(x_dhd), random_state=3, y_true=y_true, num=2))\n",
    "    debiases.append(debiased_frequency)\n",
    "\n",
    "# use debias with the lowest precision for further study\n",
    "db = debiases[np.argmin(precisions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q_DPqJB6-xx"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gdcYwXhCfeIY"
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from scipy import spatial\n",
    "\n",
    "def generateAnalogies(analogyTemplates, keyedVecs):\n",
    "\n",
    "\t# generates anaology templates from passing stereotypical roles\n",
    "\t# Ex. [['caucasian', 'manager', 'asian'], ['black', 'musician', 'caucasian'], ['asian', 'teacher', 'black']]\n",
    "\texpandedAnalogyTemplates = []\n",
    "\tfor A, stereotypes in analogyTemplates.items():\n",
    "\t\tfor B, _ in analogyTemplates.items():\n",
    "\t\t\tif(A != B):\n",
    "\t\t\t\tfor stereotype in stereotypes:\n",
    "\t\t\t\t\texpandedAnalogyTemplates.append([A, stereotype, B])\n",
    "\n",
    "\t# Test every analogy template with every word in our vocab\n",
    "\tanalogies = []\n",
    "\toutputGroups = []\n",
    "\tfor a,b,x in expandedAnalogyTemplates:\n",
    "\t\toutputs = scoredAnalogyAnswers(a,b,x,keyedVecs)\n",
    "\t\tformattedOutput = []\n",
    "\t\t\n",
    "\t\tfor score, a_w, b_w, x_w, y_w in outputs:\n",
    "\t\t\t\n",
    "\t\t\tanalogy = str(a_w) + \" is to \" + str(b_w) + \" as \" + str(x_w) + \" is to \" + str(y_w)\n",
    "\t\t\tanalogyRaw = [a_w, b_w, x_w, y_w]\n",
    "\t\t\tanalogies.append([score, analogy, analogyRaw])\n",
    "\t\t\tformattedOutput.append([score, analogy, analogyRaw])\n",
    "\t\toutputGroups.append(formattedOutput)\n",
    "\n",
    "\tanalogies = sorted(analogies, key=lambda x:-x[0])\n",
    "\treturn analogies, outputGroups\n",
    "\n",
    "def convert_legacy_to_keyvec(legacy_w2v):\n",
    "    dim = len(list(word_vectors.values())[0])\n",
    "    vectors = Word2VecKeyedVectors(dim)\n",
    "\n",
    "    ws = []\n",
    "    vs = []\n",
    "\n",
    "    for word, vect in legacy_w2v.items():\n",
    "        ws.append(word)\n",
    "        vs.append(vect)\n",
    "        assert(len(vect) == dim)\n",
    "    vectors.add(ws, vs, replace=True)\n",
    "    return vectors\n",
    "\n",
    "def scoredAnalogyAnswers(a,b,x, keyedVecs, thresh=12.5):\n",
    "\t\"\"\"\n",
    "\tcosine similarity score of the analogies\n",
    "\tcloser to 1 = the difference between a and b is very similar to\n",
    "\tthe distance between x and y\n",
    "\tEx. 'apple is to fruit as carrot is to vegetable\n",
    "\tthe vector difference between apple and fruit\n",
    "\tis compared to the difference in carrot and vegetable\n",
    "\tif they are good analogies then the cosine similarities will\n",
    "\tbe 1\n",
    "\t\"\"\"\n",
    "\n",
    "\twords = [w for w in keyedVecs.vocab if np.linalg.norm(np.array(keyedVecs[w])-np.array(keyedVecs[x])) < thresh]\n",
    "\t\n",
    "\tdef cos(a,b,x,y):\n",
    "\t\taVec = np.array(keyedVecs[a])\n",
    "\t\tbVec = np.array(keyedVecs[b])\n",
    "\t\txVec = np.array(keyedVecs[x])\n",
    "\t\tyVec = np.array(keyedVecs[y])\n",
    "\t\tnumerator = (aVec-bVec).dot(xVec-yVec)\n",
    "\t\tdenominator = np.linalg.norm(aVec-bVec)*np.linalg.norm(xVec-yVec)\n",
    "\t\treturn numerator/(denominator if denominator != 0 else 1e-6)\n",
    "\n",
    "\treturn sorted([(cos(a,b,x,y), a,b,x,y) for y in words], reverse=True)\n",
    " \n",
    "def multiclass_evaluation(embeddings, targets, attributes):\n",
    "\ttargets_eval = []\n",
    "\tfor targetSet in targets:\n",
    "\t\tfor target in targetSet:\n",
    "\t\t\tfor attributeSet in attributes:\n",
    "\t\t\t\ttargets_eval.append(_unary_s(embeddings, target, attributeSet))\n",
    "\tm_score = np.mean(targets_eval)\n",
    "\treturn m_score, targets_eval\n",
    "\n",
    "def _unary_s(embeddings, target, attributes):\n",
    "\treturn np.mean([ spatial.distance.cosine(embeddings[target], embeddings[ai]) for ai in attributes ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e_2Rs17YfyZH"
   },
   "outputs": [],
   "source": [
    "biasedAnalogies, biasedAnalogyGroups = generateAnalogies(roles, convert_legacy_to_keyvec(word_vectors))\n",
    "hardDebiasedAnalogies, hardDebiasedAnalogyGroups = generateAnalogies(roles, convert_legacy_to_keyvec(new_vocab))\n",
    "doubleHardDebiasedAnalogies, doubleHardDebiasedAnalogyGroups = generateAnalogies(roles, convert_legacy_to_keyvec(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKwGytXlqm6W",
    "outputId": "1053228f-9966-4c1d-a640-c66c14c8dfca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased Analogies (0-20)\n",
      "0.7440956387393549 asian is to laborer as caucasian is to maxim\n",
      "0.7242655048287052 asian is to laborer as caucasian is to quorum\n",
      "0.7227820072311668 asian is to engineer as caucasian is to sociologist\n",
      "0.7215993085150522 black is to runner as caucasian is to fricative\n",
      "0.714185409976964 asian is to laborer as caucasian is to forgery\n",
      "0.7123000623464896 black is to runner as caucasian is to rifleman\n",
      "0.7109950201019208 black is to runner as caucasian is to alveolar\n",
      "0.7099742338897985 black is to runner as caucasian is to para\n",
      "0.7098601539812843 black is to runner as caucasian is to infinitive\n",
      "0.7093358220823148 asian is to engineer as caucasian is to astronomer\n",
      "0.7057686454979323 asian is to engineer as caucasian is to statistician\n",
      "0.7056965451006092 asian is to laborer as caucasian is to moniker\n",
      "0.705575826351776 caucasian is to executive as asian is to australian\n",
      "0.7049933109196884 asian is to laborer as caucasian is to predicate\n",
      "0.7049448993105739 asian is to laborer as caucasian is to usury\n",
      "0.7044244363335512 asian is to laborer as caucasian is to claimant\n",
      "0.7044109848616994 black is to criminal as caucasian is to infanticide\n",
      "0.7025802206956957 black is to runner as caucasian is to gladius\n",
      "0.7024306794608385 asian is to laborer as caucasian is to landowner\n",
      "0.7023263947157877 asian is to laborer as caucasian is to excommunication\n",
      "==================== \n",
      "\n",
      "\n",
      "Hard Debiased Analogies (0-20)\n",
      "0.9456051921927736 caucasian is to hillbilly as black is to hick\n",
      "0.9419929004800542 caucasian is to redneck as black is to hick\n",
      "0.910441888729178 caucasian is to hillbilly as black is to hippy\n",
      "0.904569336186123 caucasian is to manager as black is to manager\n",
      "0.9042337715016214 caucasian is to redneck as black is to hipster\n",
      "0.9027284667628938 caucasian is to executive as black is to executive\n",
      "0.9026763402982287 black is to runner as caucasian is to runner\n",
      "0.9016549018916157 black is to criminal as caucasian is to criminal\n",
      "0.9006960028061507 black is to musician as caucasian is to musician\n",
      "0.9002105774349491 caucasian is to redneck as black is to hippy\n",
      "0.9000797728451818 caucasian is to leader as black is to leader\n",
      "0.8991755484410695 caucasian is to hillbilly as black is to skinhead\n",
      "0.8985171660036047 caucasian is to farmer as black is to farmer\n",
      "0.8977232639612308 black is to slave as caucasian is to slave\n",
      "0.8967848961461689 black is to homeless as caucasian is to homeless\n",
      "0.8936887479734643 caucasian is to hillbilly as black is to yuppie\n",
      "0.8936205986566272 caucasian is to hillbilly as black is to hillbilly\n",
      "0.8934204668492096 caucasian is to redneck as black is to yuppie\n",
      "0.8920706772942232 caucasian is to redneck as black is to redneck\n",
      "0.8871913824206343 caucasian is to hillbilly as black is to hipster\n",
      "==================== \n",
      "\n",
      "\n",
      "Double-Hard Debiased Analogies (0-20)\n",
      "0.9201367272041588 black is to musician as caucasian is to musician\n",
      "0.9199140781842333 black is to runner as caucasian is to runner\n",
      "0.9178938865156497 caucasian is to hillbilly as black is to hillbilly\n",
      "0.9155592763022052 caucasian is to executive as black is to executive\n",
      "0.9145095488566914 caucasian is to redneck as black is to redneck\n",
      "0.9111868212450052 black is to criminal as caucasian is to criminal\n",
      "0.9096124093025728 caucasian is to manager as black is to manager\n",
      "0.9092583889744633 caucasian is to farmer as black is to farmer\n",
      "0.909074033558337 black is to homeless as caucasian is to homeless\n",
      "0.9053161793662302 black is to slave as caucasian is to slave\n",
      "0.9037068480190665 caucasian is to leader as black is to leader\n",
      "0.8963587517774441 caucasian is to redneck as black is to hillbilly\n",
      "0.888305635919329 caucasian is to hillbilly as black is to redneck\n",
      "0.8842468555548904 black is to musician as caucasian is to novelist\n",
      "0.8821341613477418 black is to musician as caucasian is to composer\n",
      "0.8715295745098492 black is to musician as caucasian is to guitarist\n",
      "0.8708628142798513 black is to musician as caucasian is to programmer\n",
      "0.8690101757245352 caucasian is to hillbilly as black is to hick\n",
      "0.8689211804725802 caucasian is to redneck as black is to hick\n",
      "0.8682282866295019 black is to musician as caucasian is to footballer\n"
     ]
    }
   ],
   "source": [
    "limit = 20\n",
    "print(\"Biased Analogies (0-\" + str(limit) + \")\")\n",
    "for score, analogy, _ in biasedAnalogies[:limit]:\n",
    "    print(score, analogy)\n",
    "\n",
    "print(\"=\"*20, \"\\n\\n\")\n",
    "print(\"Hard Debiased Analogies (0-\" + str(limit) + \")\")\n",
    "for score, analogy, _ in hardDebiasedAnalogies[:limit]:\n",
    "    print(score, analogy)\n",
    "\n",
    "print(\"=\"*20, \"\\n\\n\")\n",
    "print(\"Double-Hard Debiased Analogies (0-\" + str(limit) + \")\")\n",
    "for score, analogy, _ in doubleHardDebiasedAnalogies[:limit]:\n",
    "    print(score, analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CE00WBZYsj3q",
    "outputId": "c298e57f-9010-46c4-af35-a2d99f826163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Evaluation\n",
      "====================\n",
      "Biased MAC: 0.8920888687350088\n",
      "HARD MAC: 0.9809887952002936\n",
      "DOUBLE HARD MAC: 0.9406683954862781\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing Evaluation\")\n",
    "print(\"=\"*20,)\n",
    "\n",
    "evalTargets, evalAttrs = attributes['eval_targets'], attributes[\"analogy_templates\"]['role'].values()\n",
    "\n",
    "biasedMAC, biasedDistribution = multiclass_evaluation(word_vectors, evalTargets, evalAttrs)\n",
    "print(\"Biased MAC:\", biasedMAC)\n",
    "debiasedMAC, debiasedDistribution = multiclass_evaluation(new_vocab, evalTargets, evalAttrs)\n",
    "print(\"HARD MAC:\", debiasedMAC)\n",
    "doubledebiasedMAC, doubledebiasedDistribution = multiclass_evaluation(db, evalTargets, evalAttrs)\n",
    "print(\"DOUBLE HARD MAC:\", doubledebiasedMAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPP1GXs_olTz"
   },
   "source": [
    "## Concept Categorization\n",
    "\n",
    "Cluster a set of words into different categorical subsets. Metric is fraction of the total number of the words that are correctly classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QGRDBN_sOik"
   },
   "source": [
    "## Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8e3CW5Nq0zwi"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn.utils import Bunch\n",
    "from six import iteritems\n",
    "\n",
    "def evaluate_categorization(word_vectors, X, y, method='kmeans', seed=None):\n",
    "    \"\"\"\n",
    "    Evaluate embeddings on categorization task.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w: dict\n",
    "      Embeddings to test.\n",
    "    X: vector, shape: (n_samples, )\n",
    "      Vector of words.\n",
    "    y: vector, shape: (n_samples, )\n",
    "      Vector of cluster assignments.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all word embeddings into a matrix\n",
    "    vectors = []\n",
    "    for word in word_vectors:\n",
    "        vectors.append(word_vectors[word])\n",
    "\n",
    "    # Mean of all embeddings\n",
    "    mean_vector = np.mean(vectors, axis=0, keepdims=True)\n",
    "\n",
    "    w = word_vectors\n",
    "\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    exist_cnt = 0\n",
    "    for idx, word in enumerate(X.flatten()):\n",
    "        if word in w :\n",
    "            new_x.append(X[idx])\n",
    "            new_y.append(y[idx])\n",
    "            exist_cnt += 1\n",
    "\n",
    "    # Number of words in BLESS that also exists in our vocabulary\n",
    "    print('exist {} in {}'.format(exist_cnt, len(X)))\n",
    "\n",
    "    X = np.array(new_x)\n",
    "    y = np.array(new_y)\n",
    "\n",
    "    # Put all the words that were in both BLESS and our vocab into a matrix\n",
    "    words = np.vstack([w.get(word, mean_vector) for word in X.flatten()])\n",
    "    ids = np.random.RandomState(seed).choice(range(len(X)), len(X), replace=False)\n",
    "\n",
    "    # Evaluate clustering on several hyperparameters of AgglomerativeClustering and\n",
    "    # KMeans\n",
    "    best_purity = 0\n",
    "\n",
    "    if method == \"all\" or method == \"agglomerative\":\n",
    "        best_purity = calculate_purity(y[ids], AgglomerativeClustering(n_clusters=len(set(y)),\n",
    "                                                                       affinity=\"euclidean\",\n",
    "                                                                       linkage=\"ward\").fit_predict(words[ids]))\n",
    "        for affinity in [\"cosine\", \"euclidean\"]:\n",
    "            for linkage in [\"average\", \"complete\"]:\n",
    "                purity = calculate_purity(y[ids], AgglomerativeClustering(n_clusters=len(set(y)),\n",
    "                                                                          affinity=affinity,\n",
    "                                                                          linkage=linkage).fit_predict(words[ids]))\n",
    "                best_purity = max(best_purity, purity)\n",
    "\n",
    "    if method == \"all\" or method == \"kmeans\":\n",
    "        purity = calculate_purity(y[ids], KMeans(random_state=seed, n_init=10, n_clusters=len(set(y))).\n",
    "                                  fit_predict(words[ids]))\n",
    "        best_purity = max(purity, best_purity)\n",
    "\n",
    "    return best_purity\n",
    "\n",
    "\n",
    "def calculate_purity(y_true, y_pred, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate purity for given true and predicted cluster labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array, shape: (n_samples, 1)\n",
    "      True cluster labels\n",
    "    y_pred: array, shape: (n_samples, 1)\n",
    "      Cluster assingment.\n",
    "    Returns\n",
    "    -------\n",
    "    purity: float\n",
    "      Calculated purity.\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    true_clusters = np.zeros(shape=(len(set(y_true)), len(y_true)))  # creates sparse array (categories, words both in bench and vocab)\n",
    "    pred_clusters = np.zeros_like(true_clusters)  # creates sparse array (categories, words both in bench and vocab)\n",
    "    for id, cl in enumerate(set(y_true)):\n",
    "        if verbose:\n",
    "            print(\"true:\", id)\n",
    "        true_clusters[id] = (y_true == cl).astype(\"int\")  # Everwhere the label is of a certain class, put a 1\n",
    "    for id, cl in enumerate(set(y_pred)):\n",
    "        if verbose:\n",
    "            print(\"pred:\", id)\n",
    "        pred_clusters[id] = (y_pred == cl).astype(\"int\")  # Everwhere the label is in a certain cluster, put a 1\n",
    "\n",
    "    # For each clust in the prediction, find the true cluster that has the MOST overlap\n",
    "    # Sum up the number of words that overlap between the pred and true cluster that has the most overlap\n",
    "    # Divide this by (the number of words both in bench and vocab)\n",
    "    M = pred_clusters.dot(true_clusters.T)\n",
    "    return 1. / len(y_true) * np.sum(np.max(M, axis=1))\n",
    "    \n",
    "\n",
    "def evaluate_cate(wv_dict, benchmarks, method='all', seed=None):\n",
    "    categorization_tasks = benchmarks\n",
    "    categorization_results = {}\n",
    "\n",
    "    # Calculate results using helper function\n",
    "    for name, data in iteritems(categorization_tasks):\n",
    "        print(\"Sample data from {}, num of samples: {} : \\\"{}\\\" is assigned class {}\".format(\n",
    "            name, len(data.X), data.X[0], data.y[0]))\n",
    "        categorization_results[name] = evaluate_categorization(wv_dict, data.X, data.y, method=method, seed=None)\n",
    "        print(\"Cluster purity on {} {}\".format(name, categorization_results[name]))\n",
    "\n",
    "def create_bunches(bench_paths):\n",
    "    output = {}\n",
    "    for path in bench_paths:\n",
    "        files = glob.glob(os.path.join(path, \"*.txt\"))\n",
    "        name = os.path.basename(path)\n",
    "        if name == 'EN-BATTIG':\n",
    "            sep = \",\"\n",
    "        else:\n",
    "            sep = \" \"\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        names = []\n",
    "        for cluster_id, file_name in enumerate(files):\n",
    "            with open(file_name) as f:\n",
    "                lines = f.read().splitlines()[:]\n",
    "                X += [l.split(sep) for l in lines]\n",
    "                y += [os.path.basename(file_name).split(\".\")[0]] * len(lines)\n",
    "        output[name] = Bunch(X=np.array(X, dtype=\"object\"), y=np.array(y).astype(\"object\"))\n",
    "\n",
    "        if sep == \",\":\n",
    "            data = output[name]\n",
    "            output[name] = Bunch(X=data.X[:, 0], y=data.y, freq=data.X[:, 1], frequency=data.X[:, 2], rank=data.X[:, 3], rfreq=data.X[:, 4])\n",
    "\n",
    "            \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "qil_yrAhFte0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EN-AP': {'X': array([['acne'],\n",
       "         ['anthrax'],\n",
       "         ['arthritis'],\n",
       "         ['asthma'],\n",
       "         ['cancer'],\n",
       "         ['cholera'],\n",
       "         ['cirrhosis'],\n",
       "         ['diabetes'],\n",
       "         ['eczema'],\n",
       "         ['flu'],\n",
       "         ['glaucoma'],\n",
       "         ['hepatitis'],\n",
       "         ['leukemia'],\n",
       "         ['malnutrition'],\n",
       "         ['meningitis'],\n",
       "         ['plague'],\n",
       "         ['rheumatism'],\n",
       "         ['smallpox'],\n",
       "         ['aluminium'],\n",
       "         ['bismuth'],\n",
       "         ['cadmium'],\n",
       "         ['calcium'],\n",
       "         ['carbon'],\n",
       "         ['charcoal'],\n",
       "         ['copper'],\n",
       "         ['germanium'],\n",
       "         ['helium'],\n",
       "         ['hydrogen'],\n",
       "         ['iron'],\n",
       "         ['lithium'],\n",
       "         ['magnesium'],\n",
       "         ['neon'],\n",
       "         ['nitrogen'],\n",
       "         ['oxygen'],\n",
       "         ['platinum'],\n",
       "         ['potassium'],\n",
       "         ['silver'],\n",
       "         ['titanium'],\n",
       "         ['zinc'],\n",
       "         ['compulsion'],\n",
       "         ['conscience'],\n",
       "         ['deterrence'],\n",
       "         ['disincentive'],\n",
       "         ['dynamic'],\n",
       "         ['ethics'],\n",
       "         ['impulse'],\n",
       "         ['incentive'],\n",
       "         ['incitement'],\n",
       "         ['inducement'],\n",
       "         ['life'],\n",
       "         ['mania'],\n",
       "         ['morality'],\n",
       "         ['motivator'],\n",
       "         ['obsession'],\n",
       "         ['occasion'],\n",
       "         ['possession'],\n",
       "         ['superego'],\n",
       "         ['urge'],\n",
       "         ['wanderlust'],\n",
       "         ['cent'],\n",
       "         ['cordoba'],\n",
       "         ['dinar'],\n",
       "         ['dirham'],\n",
       "         ['dollar'],\n",
       "         ['drachma'],\n",
       "         ['escudo'],\n",
       "         ['fen'],\n",
       "         ['franc'],\n",
       "         ['guilder'],\n",
       "         ['lira'],\n",
       "         ['mark'],\n",
       "         ['penny'],\n",
       "         ['peso'],\n",
       "         ['pound'],\n",
       "         ['riel'],\n",
       "         ['rouble'],\n",
       "         ['rupee'],\n",
       "         ['shilling'],\n",
       "         ['yuan'],\n",
       "         ['zloty'],\n",
       "         ['acceptance'],\n",
       "         ['assignment'],\n",
       "         ['bill'],\n",
       "         ['bond'],\n",
       "         ['check'],\n",
       "         ['cheque'],\n",
       "         ['constitution'],\n",
       "         ['convention'],\n",
       "         ['decree'],\n",
       "         ['draft'],\n",
       "         ['floater'],\n",
       "         ['law'],\n",
       "         ['licence'],\n",
       "         ['obligation'],\n",
       "         ['opinion'],\n",
       "         ['rescript'],\n",
       "         ['sequestration'],\n",
       "         ['share'],\n",
       "         ['statute'],\n",
       "         ['straddle'],\n",
       "         ['treaty'],\n",
       "         ['concavity'],\n",
       "         ['corner'],\n",
       "         ['crinkle'],\n",
       "         ['cube'],\n",
       "         ['cuboid'],\n",
       "         ['cylinder'],\n",
       "         ['dodecahedron'],\n",
       "         ['dome'],\n",
       "         ['droop'],\n",
       "         ['fluting'],\n",
       "         ['icosahedron'],\n",
       "         ['indentation'],\n",
       "         ['jag'],\n",
       "         ['knob'],\n",
       "         ['octahedron'],\n",
       "         ['ovoid'],\n",
       "         ['ring'],\n",
       "         ['salient'],\n",
       "         ['taper'],\n",
       "         ['tetrahedron'],\n",
       "         ['ball'],\n",
       "         ['celebration'],\n",
       "         ['ceremony'],\n",
       "         ['commemoration'],\n",
       "         ['commencement'],\n",
       "         ['coronation'],\n",
       "         ['dance'],\n",
       "         ['enthronement'],\n",
       "         ['feast'],\n",
       "         ['fete'],\n",
       "         ['fiesta'],\n",
       "         ['fundraiser'],\n",
       "         ['funeral'],\n",
       "         ['graduation'],\n",
       "         ['inaugural'],\n",
       "         ['pageantry'],\n",
       "         ['party'],\n",
       "         ['prom'],\n",
       "         ['rededication'],\n",
       "         ['wedding'],\n",
       "         ['baccarat'],\n",
       "         ['basketball'],\n",
       "         ['beano'],\n",
       "         ['bowling'],\n",
       "         ['chess'],\n",
       "         ['curling'],\n",
       "         ['faro'],\n",
       "         ['football'],\n",
       "         ['golf'],\n",
       "         ['handball'],\n",
       "         ['keno'],\n",
       "         ['lotto'],\n",
       "         ['nap'],\n",
       "         ['raffle'],\n",
       "         ['rugby'],\n",
       "         ['soccer'],\n",
       "         ['softball'],\n",
       "         ['tennis'],\n",
       "         ['volleyball'],\n",
       "         ['whist'],\n",
       "         ['aeon'],\n",
       "         ['date'],\n",
       "         ['day'],\n",
       "         ['epoch'],\n",
       "         ['future'],\n",
       "         ['gestation'],\n",
       "         ['hereafter'],\n",
       "         ['menopause'],\n",
       "         ['moment'],\n",
       "         ['nonce'],\n",
       "         ['period'],\n",
       "         ['quaternary'],\n",
       "         ['today'],\n",
       "         ['tomorrow'],\n",
       "         ['tonight'],\n",
       "         ['yesterday'],\n",
       "         ['yesteryear'],\n",
       "         ['anger'],\n",
       "         ['desire'],\n",
       "         ['fear'],\n",
       "         ['happiness'],\n",
       "         ['joy'],\n",
       "         ['love'],\n",
       "         ['pain'],\n",
       "         ['passion'],\n",
       "         ['pleasure'],\n",
       "         ['sadness'],\n",
       "         ['sensitivity'],\n",
       "         ['shame'],\n",
       "         ['wonder'],\n",
       "         ['ache'],\n",
       "         ['backache'],\n",
       "         ['bellyache'],\n",
       "         ['burn'],\n",
       "         ['earache'],\n",
       "         ['headache'],\n",
       "         ['lumbago'],\n",
       "         ['migraine'],\n",
       "         ['neuralgia'],\n",
       "         ['sciatica'],\n",
       "         ['soreness'],\n",
       "         ['sting'],\n",
       "         ['stinging'],\n",
       "         ['stitch'],\n",
       "         ['suffering'],\n",
       "         ['tenderness'],\n",
       "         ['throb'],\n",
       "         ['toothache'],\n",
       "         ['torment'],\n",
       "         ['architect'],\n",
       "         ['artist'],\n",
       "         ['builder'],\n",
       "         ['constructor'],\n",
       "         ['craftsman'],\n",
       "         ['designer'],\n",
       "         ['developer'],\n",
       "         ['farmer'],\n",
       "         ['inventor'],\n",
       "         ['maker'],\n",
       "         ['manufacturer'],\n",
       "         ['musician'],\n",
       "         ['originator'],\n",
       "         ['painter'],\n",
       "         ['photographer'],\n",
       "         ['producer'],\n",
       "         ['tailor'],\n",
       "         ['agency'],\n",
       "         ['branch'],\n",
       "         ['brigade'],\n",
       "         ['bureau'],\n",
       "         ['club'],\n",
       "         ['committee'],\n",
       "         ['company'],\n",
       "         ['confederacy'],\n",
       "         ['department'],\n",
       "         ['divan'],\n",
       "         ['family'],\n",
       "         ['house'],\n",
       "         ['household'],\n",
       "         ['league'],\n",
       "         ['legion'],\n",
       "         ['nation'],\n",
       "         ['office'],\n",
       "         ['platoon'],\n",
       "         ['team'],\n",
       "         ['tribe'],\n",
       "         ['troop'],\n",
       "         ['aircraft'],\n",
       "         ['airplane'],\n",
       "         ['automobile'],\n",
       "         ['bicycle'],\n",
       "         ['boat'],\n",
       "         ['car'],\n",
       "         ['cruiser'],\n",
       "         ['helicopter'],\n",
       "         ['motorcycle'],\n",
       "         ['pickup'],\n",
       "         ['rocket'],\n",
       "         ['ship'],\n",
       "         ['truck'],\n",
       "         ['van'],\n",
       "         ['airstream'],\n",
       "         ['aurora'],\n",
       "         ['blast'],\n",
       "         ['clemency'],\n",
       "         ['cloud'],\n",
       "         ['cloudburst'],\n",
       "         ['crosswind'],\n",
       "         ['cyclone'],\n",
       "         ['drizzle'],\n",
       "         ['fog'],\n",
       "         ['hurricane'],\n",
       "         ['lightning'],\n",
       "         ['rainstorm'],\n",
       "         ['sandstorm'],\n",
       "         ['shower'],\n",
       "         ['snowfall'],\n",
       "         ['thunderstorm'],\n",
       "         ['tornado'],\n",
       "         ['twister'],\n",
       "         ['typhoon'],\n",
       "         ['wind'],\n",
       "         ['bear'],\n",
       "         ['bull'],\n",
       "         ['camel'],\n",
       "         ['cat'],\n",
       "         ['cow'],\n",
       "         ['deer'],\n",
       "         ['dog'],\n",
       "         ['elephant'],\n",
       "         ['horse'],\n",
       "         ['kitten'],\n",
       "         ['lion'],\n",
       "         ['monkey'],\n",
       "         ['mouse'],\n",
       "         ['oyster'],\n",
       "         ['puppy'],\n",
       "         ['rat'],\n",
       "         ['sheep'],\n",
       "         ['tiger'],\n",
       "         ['turtle'],\n",
       "         ['zebra'],\n",
       "         ['allocation'],\n",
       "         ['allotment'],\n",
       "         ['capital'],\n",
       "         ['credit'],\n",
       "         ['dispensation'],\n",
       "         ['fund'],\n",
       "         ['gain'],\n",
       "         ['gold'],\n",
       "         ['hoard'],\n",
       "         ['income'],\n",
       "         ['interest'],\n",
       "         ['investment'],\n",
       "         ['margin'],\n",
       "         ['mortgage'],\n",
       "         ['payoff'],\n",
       "         ['profit'],\n",
       "         ['quota'],\n",
       "         ['taxation'],\n",
       "         ['trove'],\n",
       "         ['venture'],\n",
       "         ['wager'],\n",
       "         ['apple'],\n",
       "         ['banana'],\n",
       "         ['berry'],\n",
       "         ['cherry'],\n",
       "         ['grape'],\n",
       "         ['kiwi'],\n",
       "         ['lemon'],\n",
       "         ['mango'],\n",
       "         ['melon'],\n",
       "         ['olive'],\n",
       "         ['orange'],\n",
       "         ['peach'],\n",
       "         ['pear'],\n",
       "         ['pineapple'],\n",
       "         ['strawberry'],\n",
       "         ['watermelon'],\n",
       "         ['anchorage'],\n",
       "         ['borderland'],\n",
       "         ['borough'],\n",
       "         ['caliphate'],\n",
       "         ['canton'],\n",
       "         ['city'],\n",
       "         ['country'],\n",
       "         ['county'],\n",
       "         ['kingdom'],\n",
       "         ['land'],\n",
       "         ['metropolis'],\n",
       "         ['parish'],\n",
       "         ['prefecture'],\n",
       "         ['riverside'],\n",
       "         ['seafront'],\n",
       "         ['shire'],\n",
       "         ['state'],\n",
       "         ['suburb'],\n",
       "         ['sultanate'],\n",
       "         ['town'],\n",
       "         ['village'],\n",
       "         ['acacia'],\n",
       "         ['casuarina'],\n",
       "         ['chestnut'],\n",
       "         ['cinchona'],\n",
       "         ['coco'],\n",
       "         ['conifer'],\n",
       "         ['fig'],\n",
       "         ['hornbeam'],\n",
       "         ['jacaranda'],\n",
       "         ['lime'],\n",
       "         ['mandarin'],\n",
       "         ['mangrove'],\n",
       "         ['oak'],\n",
       "         ['palm'],\n",
       "         ['pine'],\n",
       "         ['pistachio'],\n",
       "         ['rowan'],\n",
       "         ['samba'],\n",
       "         ['sapling'],\n",
       "         ['sycamore'],\n",
       "         ['walnut'],\n",
       "         ['chill'],\n",
       "         ['coolness'],\n",
       "         ['deflection'],\n",
       "         ['diameter'],\n",
       "         ['extension'],\n",
       "         ['glow'],\n",
       "         ['heaviness'],\n",
       "         ['length'],\n",
       "         ['mass'],\n",
       "         ['momentum'],\n",
       "         ['plasticity'],\n",
       "         ['poundage'],\n",
       "         ['radius'],\n",
       "         ['reflexion'],\n",
       "         ['shortness'],\n",
       "         ['snap'],\n",
       "         ['stretch'],\n",
       "         ['temperature'],\n",
       "         ['visibility'],\n",
       "         ['weight']], dtype=object),\n",
       "  'y': array(['illness', 'illness', 'illness', 'illness', 'illness', 'illness',\n",
       "         'illness', 'illness', 'illness', 'illness', 'illness', 'illness',\n",
       "         'illness', 'illness', 'illness', 'illness', 'illness', 'illness',\n",
       "         'chemical_element', 'chemical_element', 'chemical_element',\n",
       "         'chemical_element', 'chemical_element', 'chemical_element',\n",
       "         'chemical_element', 'chemical_element', 'chemical_element',\n",
       "         'chemical_element', 'chemical_element', 'chemical_element',\n",
       "         'chemical_element', 'chemical_element', 'chemical_element',\n",
       "         'chemical_element', 'chemical_element', 'chemical_element',\n",
       "         'chemical_element', 'chemical_element', 'chemical_element',\n",
       "         'motivation', 'motivation', 'motivation', 'motivation',\n",
       "         'motivation', 'motivation', 'motivation', 'motivation',\n",
       "         'motivation', 'motivation', 'motivation', 'motivation',\n",
       "         'motivation', 'motivation', 'motivation', 'motivation',\n",
       "         'motivation', 'motivation', 'motivation', 'motivation',\n",
       "         'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "         'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "         'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "         'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "         'monetary_unit', 'monetary_unit', 'monetary_unit', 'monetary_unit',\n",
       "         'monetary_unit', 'legal_document', 'legal_document',\n",
       "         'legal_document', 'legal_document', 'legal_document',\n",
       "         'legal_document', 'legal_document', 'legal_document',\n",
       "         'legal_document', 'legal_document', 'legal_document',\n",
       "         'legal_document', 'legal_document', 'legal_document',\n",
       "         'legal_document', 'legal_document', 'legal_document',\n",
       "         'legal_document', 'legal_document', 'legal_document',\n",
       "         'legal_document', 'solid', 'solid', 'solid', 'solid', 'solid',\n",
       "         'solid', 'solid', 'solid', 'solid', 'solid', 'solid', 'solid',\n",
       "         'solid', 'solid', 'solid', 'solid', 'solid', 'solid', 'solid',\n",
       "         'solid', 'social_occasion', 'social_occasion', 'social_occasion',\n",
       "         'social_occasion', 'social_occasion', 'social_occasion',\n",
       "         'social_occasion', 'social_occasion', 'social_occasion',\n",
       "         'social_occasion', 'social_occasion', 'social_occasion',\n",
       "         'social_occasion', 'social_occasion', 'social_occasion',\n",
       "         'social_occasion', 'social_occasion', 'social_occasion',\n",
       "         'social_occasion', 'social_occasion', 'game', 'game', 'game',\n",
       "         'game', 'game', 'game', 'game', 'game', 'game', 'game', 'game',\n",
       "         'game', 'game', 'game', 'game', 'game', 'game', 'game', 'game',\n",
       "         'game', 'time', 'time', 'time', 'time', 'time', 'time', 'time',\n",
       "         'time', 'time', 'time', 'time', 'time', 'time', 'time', 'time',\n",
       "         'time', 'time', 'feeling', 'feeling', 'feeling', 'feeling',\n",
       "         'feeling', 'feeling', 'feeling', 'feeling', 'feeling', 'feeling',\n",
       "         'feeling', 'feeling', 'feeling', 'pain', 'pain', 'pain', 'pain',\n",
       "         'pain', 'pain', 'pain', 'pain', 'pain', 'pain', 'pain', 'pain',\n",
       "         'pain', 'pain', 'pain', 'pain', 'pain', 'pain', 'pain', 'creator',\n",
       "         'creator', 'creator', 'creator', 'creator', 'creator', 'creator',\n",
       "         'creator', 'creator', 'creator', 'creator', 'creator', 'creator',\n",
       "         'creator', 'creator', 'creator', 'creator', 'social_unit',\n",
       "         'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "         'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "         'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "         'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "         'social_unit', 'social_unit', 'social_unit', 'social_unit',\n",
       "         'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle',\n",
       "         'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle',\n",
       "         'vehicle', 'vehicle', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon',\n",
       "         'atmospheric_phenomenon', 'atmospheric_phenomenon', 'animal',\n",
       "         'animal', 'animal', 'animal', 'animal', 'animal', 'animal',\n",
       "         'animal', 'animal', 'animal', 'animal', 'animal', 'animal',\n",
       "         'animal', 'animal', 'animal', 'animal', 'animal', 'animal',\n",
       "         'animal', 'assets', 'assets', 'assets', 'assets', 'assets',\n",
       "         'assets', 'assets', 'assets', 'assets', 'assets', 'assets',\n",
       "         'assets', 'assets', 'assets', 'assets', 'assets', 'assets',\n",
       "         'assets', 'assets', 'assets', 'assets', 'edible_fruit',\n",
       "         'edible_fruit', 'edible_fruit', 'edible_fruit', 'edible_fruit',\n",
       "         'edible_fruit', 'edible_fruit', 'edible_fruit', 'edible_fruit',\n",
       "         'edible_fruit', 'edible_fruit', 'edible_fruit', 'edible_fruit',\n",
       "         'edible_fruit', 'edible_fruit', 'edible_fruit', 'district',\n",
       "         'district', 'district', 'district', 'district', 'district',\n",
       "         'district', 'district', 'district', 'district', 'district',\n",
       "         'district', 'district', 'district', 'district', 'district',\n",
       "         'district', 'district', 'district', 'district', 'district', 'tree',\n",
       "         'tree', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree',\n",
       "         'tree', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree',\n",
       "         'tree', 'tree', 'tree', 'tree', 'physical_property',\n",
       "         'physical_property', 'physical_property', 'physical_property',\n",
       "         'physical_property', 'physical_property', 'physical_property',\n",
       "         'physical_property', 'physical_property', 'physical_property',\n",
       "         'physical_property', 'physical_property', 'physical_property',\n",
       "         'physical_property', 'physical_property', 'physical_property',\n",
       "         'physical_property', 'physical_property', 'physical_property',\n",
       "         'physical_property'], dtype=object)},\n",
       " 'EN-ESSLI-2c': {'X': array([['acquire'],\n",
       "         ['lend'],\n",
       "         ['buy'],\n",
       "         ['sell'],\n",
       "         ['pay'],\n",
       "         ['evaluate'],\n",
       "         ['remember'],\n",
       "         ['know'],\n",
       "         ['forget'],\n",
       "         ['check'],\n",
       "         ['suggest'],\n",
       "         ['talk'],\n",
       "         ['speak'],\n",
       "         ['request'],\n",
       "         ['read'],\n",
       "         ['listen'],\n",
       "         ['smell'],\n",
       "         ['feel'],\n",
       "         ['look'],\n",
       "         ['notice'],\n",
       "         ['kill'],\n",
       "         ['destroy'],\n",
       "         ['repair'],\n",
       "         ['die'],\n",
       "         ['break'],\n",
       "         ['carry'],\n",
       "         ['push'],\n",
       "         ['move'],\n",
       "         ['send'],\n",
       "         ['pull'],\n",
       "         ['run'],\n",
       "         ['fly'],\n",
       "         ['drive'],\n",
       "         ['walk'],\n",
       "         ['ride'],\n",
       "         ['arrive'],\n",
       "         ['enter'],\n",
       "         ['fall'],\n",
       "         ['rise'],\n",
       "         ['leave'],\n",
       "         ['eat'],\n",
       "         ['breathe'],\n",
       "         ['drink'],\n",
       "         ['smile'],\n",
       "         ['cry']], dtype=object),\n",
       "  'y': array(['exchange-exchange', 'exchange-exchange', 'exchange-exchange',\n",
       "         'exchange-exchange', 'exchange-exchange', 'mentalState-cognition',\n",
       "         'mentalState-cognition', 'mentalState-cognition',\n",
       "         'mentalState-cognition', 'mentalState-cognition',\n",
       "         'communication-cognition', 'communication-cognition',\n",
       "         'communication-cognition', 'communication-cognition',\n",
       "         'communication-cognition', 'bodySense-body', 'bodySense-body',\n",
       "         'bodySense-body', 'bodySense-body', 'bodySense-body',\n",
       "         'changeState-changeState', 'changeState-changeState',\n",
       "         'changeState-changeState', 'changeState-changeState',\n",
       "         'changeState-changeState', 'changeLocation-motion',\n",
       "         'changeLocation-motion', 'changeLocation-motion',\n",
       "         'changeLocation-motion', 'changeLocation-motion',\n",
       "         'motionManner-motion', 'motionManner-motion',\n",
       "         'motionManner-motion', 'motionManner-motion',\n",
       "         'motionManner-motion', 'motionDirection-motion',\n",
       "         'motionDirection-motion', 'motionDirection-motion',\n",
       "         'motionDirection-motion', 'motionDirection-motion',\n",
       "         'bodyAction-body', 'bodyAction-body', 'bodyAction-body',\n",
       "         'bodyAction-body', 'bodyAction-body'], dtype=object)},\n",
       " 'EN-ESSLI-2b': {'X': array([['chicken'],\n",
       "         ['eagle'],\n",
       "         ['lion'],\n",
       "         ['turtle'],\n",
       "         ['banana'],\n",
       "         ['onion'],\n",
       "         ['potato'],\n",
       "         ['bowl'],\n",
       "         ['pencil'],\n",
       "         ['telephone'],\n",
       "         ['truck'],\n",
       "         ['ship'],\n",
       "         ['car'],\n",
       "         ['bottle'],\n",
       "         ['hammer'],\n",
       "         ['pollution'],\n",
       "         ['invitation'],\n",
       "         ['shape'],\n",
       "         ['empire'],\n",
       "         ['foundation'],\n",
       "         ['fight'],\n",
       "         ['smell'],\n",
       "         ['ache'],\n",
       "         ['ceremony'],\n",
       "         ['weather'],\n",
       "         ['jealousy'],\n",
       "         ['truth'],\n",
       "         ['hypothesis'],\n",
       "         ['hope'],\n",
       "         ['mercy'],\n",
       "         ['mystery'],\n",
       "         ['gratitude'],\n",
       "         ['concept'],\n",
       "         ['temptation'],\n",
       "         ['pride'],\n",
       "         ['belief'],\n",
       "         ['insight'],\n",
       "         ['wisdom'],\n",
       "         ['luck'],\n",
       "         ['distraction']], dtype=object),\n",
       "  'y': array(['HI', 'HI', 'HI', 'HI', 'HI', 'HI', 'HI', 'HI', 'HI', 'HI', 'HI',\n",
       "         'HI', 'HI', 'HI', 'HI', 'ME', 'ME', 'ME', 'ME', 'ME', 'ME', 'ME',\n",
       "         'ME', 'ME', 'ME', 'LO', 'LO', 'LO', 'LO', 'LO', 'LO', 'LO', 'LO',\n",
       "         'LO', 'LO', 'LO', 'LO', 'LO', 'LO', 'LO'], dtype=object)},\n",
       " 'EN-ESSLI-1a': {'X': array([['bottle'],\n",
       "         ['pencil'],\n",
       "         ['pen'],\n",
       "         ['cup'],\n",
       "         ['bowl'],\n",
       "         ['scissors'],\n",
       "         ['kettle'],\n",
       "         ['knife'],\n",
       "         ['screwdriver'],\n",
       "         ['hammer'],\n",
       "         ['spoon'],\n",
       "         ['chisel'],\n",
       "         ['telephone'],\n",
       "         ['chicken'],\n",
       "         ['eagle'],\n",
       "         ['duck'],\n",
       "         ['swan'],\n",
       "         ['owl'],\n",
       "         ['penguin'],\n",
       "         ['peacock'],\n",
       "         ['cherry'],\n",
       "         ['banana'],\n",
       "         ['pear'],\n",
       "         ['pineapple'],\n",
       "         ['mushroom'],\n",
       "         ['corn'],\n",
       "         ['lettuce'],\n",
       "         ['potato'],\n",
       "         ['onion'],\n",
       "         ['dog'],\n",
       "         ['elephant'],\n",
       "         ['cow'],\n",
       "         ['cat'],\n",
       "         ['lion'],\n",
       "         ['pig'],\n",
       "         ['snail'],\n",
       "         ['turtle'],\n",
       "         ['boat'],\n",
       "         ['car'],\n",
       "         ['ship'],\n",
       "         ['truck'],\n",
       "         ['rocket'],\n",
       "         ['motorcycle'],\n",
       "         ['helicopter']], dtype=object),\n",
       "  'y': array(['tool-artifact', 'tool-artifact', 'tool-artifact', 'tool-artifact',\n",
       "         'tool-artifact', 'tool-artifact', 'tool-artifact', 'tool-artifact',\n",
       "         'tool-artifact', 'tool-artifact', 'tool-artifact', 'tool-artifact',\n",
       "         'tool-artifact', 'bird-animal', 'bird-animal', 'bird-animal',\n",
       "         'bird-animal', 'bird-animal', 'bird-animal', 'bird-animal',\n",
       "         'fruitTree-vegetable', 'fruitTree-vegetable',\n",
       "         'fruitTree-vegetable', 'fruitTree-vegetable', 'green-vegetable',\n",
       "         'green-vegetable', 'green-vegetable', 'green-vegetable',\n",
       "         'green-vegetable', 'groundAnimal-animal', 'groundAnimal-animal',\n",
       "         'groundAnimal-animal', 'groundAnimal-animal',\n",
       "         'groundAnimal-animal', 'groundAnimal-animal',\n",
       "         'groundAnimal-animal', 'groundAnimal-animal', 'vehicle-artifact',\n",
       "         'vehicle-artifact', 'vehicle-artifact', 'vehicle-artifact',\n",
       "         'vehicle-artifact', 'vehicle-artifact', 'vehicle-artifact'],\n",
       "        dtype=object)},\n",
       " 'EN-BATTIG': {'X': array(['word', 'doll', 'ball', ..., 'pretty', 'rhododendron', 'snowball'],\n",
       "        dtype=object),\n",
       "  'y': array(['toy', 'toy', 'toy', ..., 'flower', 'flower', 'flower'],\n",
       "        dtype=object),\n",
       "  'freq': array(['freq', '285', '212', ..., '2', '2', '2'], dtype=object),\n",
       "  'frequency': array(['frequency', '10', '110', ..., 'NA', 'NA', 'NA'], dtype=object),\n",
       "  'rank': array(['rank', '1', '2', ..., '70', '71', '72'], dtype=object),\n",
       "  'rfreq': array(['rfreq', '2.88', '2.71', ..., 'NA', 'NA', 'NA'], dtype=object)},\n",
       " 'EN-BLESS': {'X': array([['carp'],\n",
       "         ['catfish'],\n",
       "         ['cod'],\n",
       "         ['dolphin'],\n",
       "         ['goldfish'],\n",
       "         ['herring'],\n",
       "         ['mackerel'],\n",
       "         ['salmon'],\n",
       "         ['trout'],\n",
       "         ['tuna'],\n",
       "         ['whale'],\n",
       "         ['crow'],\n",
       "         ['dove'],\n",
       "         ['eagle'],\n",
       "         ['falcon'],\n",
       "         ['goose'],\n",
       "         ['hawk'],\n",
       "         ['owl'],\n",
       "         ['penguin'],\n",
       "         ['pheasant'],\n",
       "         ['pigeon'],\n",
       "         ['robin'],\n",
       "         ['sparrow'],\n",
       "         ['swan'],\n",
       "         ['vulture'],\n",
       "         ['woodpecker'],\n",
       "         ['bear'],\n",
       "         ['beaver'],\n",
       "         ['bull'],\n",
       "         ['cat'],\n",
       "         ['cow'],\n",
       "         ['coyote'],\n",
       "         ['deer'],\n",
       "         ['donkey'],\n",
       "         ['elephant'],\n",
       "         ['fox'],\n",
       "         ['giraffe'],\n",
       "         ['goat'],\n",
       "         ['gorilla'],\n",
       "         ['horse'],\n",
       "         ['lion'],\n",
       "         ['pig'],\n",
       "         ['rabbit'],\n",
       "         ['rat'],\n",
       "         ['sheep'],\n",
       "         ['squirrel'],\n",
       "         ['tiger'],\n",
       "         ['beet'],\n",
       "         ['broccoli'],\n",
       "         ['cabbage'],\n",
       "         ['carrot'],\n",
       "         ['cauliflower'],\n",
       "         ['celery'],\n",
       "         ['corn'],\n",
       "         ['cucumber'],\n",
       "         ['garlic'],\n",
       "         ['lettuce'],\n",
       "         ['onion'],\n",
       "         ['parsley'],\n",
       "         ['potato'],\n",
       "         ['radish'],\n",
       "         ['spinach'],\n",
       "         ['turnip'],\n",
       "         ['dishwasher'],\n",
       "         ['freezer'],\n",
       "         ['fridge'],\n",
       "         ['oven'],\n",
       "         ['phone'],\n",
       "         ['radio'],\n",
       "         ['stereo'],\n",
       "         ['stove'],\n",
       "         ['television'],\n",
       "         ['toaster'],\n",
       "         ['washer'],\n",
       "         ['axe'],\n",
       "         ['chisel'],\n",
       "         ['corkscrew'],\n",
       "         ['fork'],\n",
       "         ['hammer'],\n",
       "         ['hatchet'],\n",
       "         ['knife'],\n",
       "         ['rake'],\n",
       "         ['saw'],\n",
       "         ['screwdriver'],\n",
       "         ['shovel'],\n",
       "         ['sieve'],\n",
       "         ['spade'],\n",
       "         ['spoon'],\n",
       "         ['wrench'],\n",
       "         ['bag'],\n",
       "         ['bottle'],\n",
       "         ['bowl'],\n",
       "         ['box'],\n",
       "         ['jar'],\n",
       "         ['mug'],\n",
       "         ['apple'],\n",
       "         ['apricot'],\n",
       "         ['banana'],\n",
       "         ['cherry'],\n",
       "         ['coconut'],\n",
       "         ['cranberry'],\n",
       "         ['grapefruit'],\n",
       "         ['grape'],\n",
       "         ['lemon'],\n",
       "         ['lime'],\n",
       "         ['peach'],\n",
       "         ['pear'],\n",
       "         ['pineapple'],\n",
       "         ['plum'],\n",
       "         ['strawberry'],\n",
       "         ['blouse'],\n",
       "         ['cloak'],\n",
       "         ['coat'],\n",
       "         ['dress'],\n",
       "         ['glove'],\n",
       "         ['hat'],\n",
       "         ['jacket'],\n",
       "         ['robe'],\n",
       "         ['scarf'],\n",
       "         ['shirt'],\n",
       "         ['sweater'],\n",
       "         ['vest'],\n",
       "         ['ambulance'],\n",
       "         ['battleship'],\n",
       "         ['bomber'],\n",
       "         ['bus'],\n",
       "         ['car'],\n",
       "         ['ferry'],\n",
       "         ['fighter'],\n",
       "         ['frigate'],\n",
       "         ['glider'],\n",
       "         ['helicopter'],\n",
       "         ['jet'],\n",
       "         ['motorcycle'],\n",
       "         ['scooter'],\n",
       "         ['tanker'],\n",
       "         ['train'],\n",
       "         ['truck'],\n",
       "         ['van'],\n",
       "         ['yacht'],\n",
       "         ['cello'],\n",
       "         ['clarinet'],\n",
       "         ['flute'],\n",
       "         ['guitar'],\n",
       "         ['piano'],\n",
       "         ['saxophone'],\n",
       "         ['trumpet'],\n",
       "         ['violin'],\n",
       "         ['bed'],\n",
       "         ['bookcase'],\n",
       "         ['chair'],\n",
       "         ['couch'],\n",
       "         ['desk'],\n",
       "         ['dresser'],\n",
       "         ['sofa'],\n",
       "         ['table'],\n",
       "         ['wardrobe'],\n",
       "         ['castle'],\n",
       "         ['cathedral'],\n",
       "         ['cottage'],\n",
       "         ['hospital'],\n",
       "         ['hotel'],\n",
       "         ['library'],\n",
       "         ['pub'],\n",
       "         ['restaurant'],\n",
       "         ['villa'],\n",
       "         ['bomb'],\n",
       "         ['cannon'],\n",
       "         ['dagger'],\n",
       "         ['grenade'],\n",
       "         ['gun'],\n",
       "         ['missile'],\n",
       "         ['musket'],\n",
       "         ['pistol'],\n",
       "         ['revolver'],\n",
       "         ['rifle'],\n",
       "         ['spear'],\n",
       "         ['sword'],\n",
       "         ['alligator'],\n",
       "         ['frog'],\n",
       "         ['lizard'],\n",
       "         ['snake'],\n",
       "         ['turtle'],\n",
       "         ['ant'],\n",
       "         ['beetle'],\n",
       "         ['butterfly'],\n",
       "         ['cockroach'],\n",
       "         ['grasshopper'],\n",
       "         ['hornet'],\n",
       "         ['moth'],\n",
       "         ['wasp'],\n",
       "         ['acacia'],\n",
       "         ['birch'],\n",
       "         ['cedar'],\n",
       "         ['cypress'],\n",
       "         ['elm'],\n",
       "         ['oak'],\n",
       "         ['pine'],\n",
       "         ['poplar'],\n",
       "         ['willow']], dtype=object),\n",
       "  'y': array(['water_animal', 'water_animal', 'water_animal', 'water_animal',\n",
       "         'water_animal', 'water_animal', 'water_animal', 'water_animal',\n",
       "         'water_animal', 'water_animal', 'water_animal', 'bird', 'bird',\n",
       "         'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird', 'bird',\n",
       "         'bird', 'bird', 'bird', 'bird', 'bird', 'ground_mammal',\n",
       "         'ground_mammal', 'ground_mammal', 'ground_mammal', 'ground_mammal',\n",
       "         'ground_mammal', 'ground_mammal', 'ground_mammal', 'ground_mammal',\n",
       "         'ground_mammal', 'ground_mammal', 'ground_mammal', 'ground_mammal',\n",
       "         'ground_mammal', 'ground_mammal', 'ground_mammal', 'ground_mammal',\n",
       "         'ground_mammal', 'ground_mammal', 'ground_mammal', 'ground_mammal',\n",
       "         'vegetable', 'vegetable', 'vegetable', 'vegetable', 'vegetable',\n",
       "         'vegetable', 'vegetable', 'vegetable', 'vegetable', 'vegetable',\n",
       "         'vegetable', 'vegetable', 'vegetable', 'vegetable', 'vegetable',\n",
       "         'vegetable', 'appliance', 'appliance', 'appliance', 'appliance',\n",
       "         'appliance', 'appliance', 'appliance', 'appliance', 'appliance',\n",
       "         'appliance', 'appliance', 'tool', 'tool', 'tool', 'tool', 'tool',\n",
       "         'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool', 'tool',\n",
       "         'tool', 'tool', 'container', 'container', 'container', 'container',\n",
       "         'container', 'container', 'fruit', 'fruit', 'fruit', 'fruit',\n",
       "         'fruit', 'fruit', 'fruit', 'fruit', 'fruit', 'fruit', 'fruit',\n",
       "         'fruit', 'fruit', 'fruit', 'fruit', 'clothing', 'clothing',\n",
       "         'clothing', 'clothing', 'clothing', 'clothing', 'clothing',\n",
       "         'clothing', 'clothing', 'clothing', 'clothing', 'clothing',\n",
       "         'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle',\n",
       "         'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle',\n",
       "         'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle', 'vehicle',\n",
       "         'musical_instrument', 'musical_instrument', 'musical_instrument',\n",
       "         'musical_instrument', 'musical_instrument', 'musical_instrument',\n",
       "         'musical_instrument', 'musical_instrument', 'furniture',\n",
       "         'furniture', 'furniture', 'furniture', 'furniture', 'furniture',\n",
       "         'furniture', 'furniture', 'furniture', 'building', 'building',\n",
       "         'building', 'building', 'building', 'building', 'building',\n",
       "         'building', 'building', 'weapon', 'weapon', 'weapon', 'weapon',\n",
       "         'weapon', 'weapon', 'weapon', 'weapon', 'weapon', 'weapon',\n",
       "         'weapon', 'weapon', 'amphibian_reptile', 'amphibian_reptile',\n",
       "         'amphibian_reptile', 'amphibian_reptile', 'amphibian_reptile',\n",
       "         'insect', 'insect', 'insect', 'insect', 'insect', 'insect',\n",
       "         'insect', 'insect', 'tree', 'tree', 'tree', 'tree', 'tree', 'tree',\n",
       "         'tree', 'tree', 'tree'], dtype=object)}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_names = [\"EN-AP\", \"EN-ESSLI-2c\", \"EN-ESSLI-2b\", \"EN-ESSLI-1a\", \"EN-BATTIG\", \"EN-BLESS\", ]\n",
    "bench_paths = [os.path.join('data', 'concept_clustering', name) for name in bench_names]\n",
    "benchmarks = create_bunches(bench_paths)\n",
    "benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cjInIq5Nx88",
    "outputId": "3ad3f6f7-4995-43ae-d4f4-0de707deb772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5091463414634146\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5555555555555556\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.6363636363636364\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.3448576409064497\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.5181347150259068\n",
      "\n",
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5945121951219512\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5555555555555556\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.6818181818181819\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.4291109819872167\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.5854922279792746\n",
      "\n",
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5975609756097561\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5333333333333333\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.7045454545454546\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.4084834398605462\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.6062176165803109\n"
     ]
    }
   ],
   "source": [
    "evaluate_cate(word_vectors, benchmarks)\n",
    "print()\n",
    "evaluate_cate(new_vocab, benchmarks)\n",
    "print()\n",
    "evaluate_cate(db, benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oykWwAIQDGd7"
   },
   "source": [
    "Control:\n",
    "\n",
    "Embeddings given to us by the Black is to Criminal Paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMCsQT9iP3Jn",
    "outputId": "49f57c85-8a90-4a5e-c2c2-4795d4e7fa2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5091463414634146\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5555555555555556\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.7045454545454546\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.341952353282975\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.5181347150259068\n",
      "\n",
      "Sample data from EN-AP, num of samples: 402 : \"['acne']\" is assigned class illness\n",
      "exist 328 in 402\n",
      "Cluster purity on EN-AP 0.5914634146341463\n",
      "Sample data from EN-ESSLI-2c, num of samples: 45 : \"['acquire']\" is assigned class exchange-exchange\n",
      "exist 45 in 45\n",
      "Cluster purity on EN-ESSLI-2c 0.5333333333333333\n",
      "Sample data from EN-ESSLI-2b, num of samples: 40 : \"['chicken']\" is assigned class HI\n",
      "exist 40 in 40\n",
      "Cluster purity on EN-ESSLI-2b 0.8500000000000001\n",
      "Sample data from EN-ESSLI-1a, num of samples: 44 : \"['bottle']\" is assigned class tool-artifact\n",
      "exist 44 in 44\n",
      "Cluster purity on EN-ESSLI-1a 0.7272727272727273\n",
      "Sample data from EN-BATTIG, num of samples: 5287 : \"word\" is assigned class toy\n",
      "exist 3442 in 5287\n",
      "Cluster purity on EN-BATTIG 0.4166182452062754\n",
      "Sample data from EN-BLESS, num of samples: 200 : \"['carp']\" is assigned class water_animal\n",
      "exist 193 in 200\n",
      "Cluster purity on EN-BLESS 0.6269430051813472\n"
     ]
    }
   ],
   "source": [
    "biased_path = os.path.join('data', 'data_vocab_race_pre_trained.w2v')\n",
    "biased_embeddings, embedding_dim = load_legacy_w2v(biased_path, 50)\n",
    "# take out words that do not only contain letters\n",
    "biased_word_vectors = pruneWordVecs(biased_embeddings)\n",
    "evaluate_cate(biased_word_vectors, benchmarks)\n",
    "\n",
    "print()\n",
    "\n",
    "debiased_path = os.path.join('data','data_vocab_race_hard_debias.w2v')\n",
    "debiased_embeddings, embedding_dim = load_legacy_w2v(debiased_path, 50)\n",
    "# take out words that do not only contain letters\n",
    "debiased_word_vectors = pruneWordVecs(debiased_embeddings)\n",
    "evaluate_cate(debiased_word_vectors, benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YelvhkXwR6tP"
   },
   "source": [
    "Conclusion:\n",
    "\n",
    "Utility is preserved\n",
    "*   AP - goes up\n",
    "*   ESSLI-2c - stays the same\n",
    "*   ESSLI-2b - stays the same\n",
    "*   ESSLI-1a - goes up then back down\n",
    "*   BATTIG - goes up (sometimes goes back down)\n",
    "*   BLESS - goes up (sometimes goes back down)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D00ZeRmvW5v"
   },
   "source": [
    "# Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DyP17ZSg0NyT"
   },
   "outputs": [],
   "source": [
    "# Create text file for each category.\n",
    "with open(os.path.join('data', 'analogies', 'EN-GOOGLE', 'EN-GOOGLE.txt'), \"r\") as file:\n",
    "        lines = file.read().splitlines()\n",
    "\n",
    "        questions = []\n",
    "        answers = []\n",
    "        category = []\n",
    "        dummy = os.path.join('data', 'analogies', 'dummy.txt')\n",
    "\n",
    "        f = open(dummy, \"w\")\n",
    "        f.write(\"/n\")\n",
    "        for line in lines:\n",
    "            if line.startswith(\":\"):\n",
    "                \n",
    "                # delete last \"/n\" and close file\n",
    "                f.seek(f.tell()-1)\n",
    "                f.truncate()\n",
    "                f.close()\n",
    "\n",
    "                # start new file for new category\n",
    "                cat = line.lower().split()[1]\n",
    "                f = open(os.path.join('data', 'analogies', f'{cat}.txt'), \"w\")\n",
    "\n",
    "            else:\n",
    "                f.write(line+'\\n')\n",
    "\n",
    "        os.remove(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tN2FUSX6_1hM"
   },
   "outputs": [],
   "source": [
    "def evaluate_analogies(word2vec_dict):\n",
    "    # Get all word embeddings into a matrix\n",
    "    vectors = []\n",
    "    for word in word2vec_dict:\n",
    "        vectors.append(word2vec_dict[word])\n",
    "    wv = np.array(vectors)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    W = np.zeros(wv.shape)\n",
    "    d = (np.sum(wv ** 2, 1) ** (0.5))\n",
    "    W = (wv.T / d).T\n",
    "\n",
    "    # Create word to index dictionary\n",
    "    vocab = {word:i for i, word in enumerate(word2vec_dict)}\n",
    "\n",
    "    filenames = [\n",
    "            'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
    "            'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
    "            'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
    "            'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
    "            'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
    "            ]\n",
    "    prefix = os.path.join('data', 'analogies')\n",
    "\n",
    "    # to avoid memory overflow, could be increased/decreased\n",
    "    # depending on system and vocab size\n",
    "    split_size = 100\n",
    "\n",
    "    correct_sem = 0; # count correct semantic questions\n",
    "    correct_syn = 0; # count correct syntactic questions\n",
    "    correct_tot = 0 # count correct questions\n",
    "    count_sem = 0; # count all semantic questions\n",
    "    count_syn = 0; # count all syntactic questions\n",
    "    count_tot = 0 # count all questions\n",
    "    full_count = 0 # count all questions, including those with unknown words\n",
    "\n",
    "    for i in range(len(filenames)):\n",
    "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
    "            full_data = [line.rstrip().split(' ') for line in f]\n",
    "            full_count += len(full_data)\n",
    "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
    "            \n",
    "        if data:\n",
    "            indices = np.array([[vocab[word] for word in row] for row in data])\n",
    "            ind1, ind2, ind3, ind4 = indices.T\n",
    "\n",
    "            predictions = np.zeros((len(indices),))\n",
    "            num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
    "            for j in range(num_iter):\n",
    "                subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
    "\n",
    "                pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
    "                    +  W[ind3[subset], :])\n",
    "                #cosine similarity if input W has been normalized\n",
    "                dist = np.dot(W, pred_vec.T)\n",
    "\n",
    "                for k in range(len(subset)):\n",
    "                    dist[ind1[subset[k]], k] = -np.Inf\n",
    "                    dist[ind2[subset[k]], k] = -np.Inf\n",
    "                    dist[ind3[subset[k]], k] = -np.Inf\n",
    "\n",
    "                # predicted word index\n",
    "                predictions[subset] = np.argmax(dist, 0).flatten()\n",
    "\n",
    "            val = (ind4 == predictions) # correct predictions\n",
    "            count_tot = count_tot + len(ind1)\n",
    "            correct_tot = correct_tot + sum(val)\n",
    "            if i < 5:\n",
    "                count_sem = count_sem + len(ind1)\n",
    "                correct_sem = correct_sem + sum(val)\n",
    "            else:\n",
    "                count_syn = count_syn + len(ind1)\n",
    "                correct_syn = correct_syn + sum(val)\n",
    "\n",
    "            print(\"%s:\" % filenames[i])\n",
    "            print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
    "                (np.mean(val) * 100, np.sum(val), len(val)))\n",
    "\n",
    "    print('Questions seen/total: %.2f%% (%d/%d)' %\n",
    "        (100 * count_tot / float(full_count), count_tot, full_count))\n",
    "    print('Semantic accuracy: %.2f%%  (%i/%i)' %\n",
    "        (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
    "    print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n",
    "        (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
    "    print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRiELeLn6CnS",
    "outputId": "6ddae7d5-4e25-4677-d523-fffb292553a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Biased ----------\n",
      "family.txt:\n",
      "ACCURACY TOP1: 70.26% (267/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 8.06% (75/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 17.86% (135/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 68.62% (914/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 40.91% (432/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 64.68% (683/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 48.85% (762/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 33.65% (424/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.14% (320/570)\n",
      "Questions seen/total: 46.25% (8900/19244)\n",
      "Semantic accuracy: 70.26%  (267/380)\n",
      "Syntactic accuracy: 43.96%  (3745/8520)\n",
      "Total accuracy: 45.08%  (4012/8900)\n",
      "\n",
      "---------- Hard Biased ----------\n",
      "family.txt:\n",
      "ACCURACY TOP1: 70.26% (267/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 8.06% (75/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 17.86% (135/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 68.62% (914/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 40.91% (432/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 64.68% (683/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 48.85% (762/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 33.65% (424/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.14% (320/570)\n",
      "Questions seen/total: 46.25% (8900/19244)\n",
      "Semantic accuracy: 70.26%  (267/380)\n",
      "Syntactic accuracy: 43.96%  (3745/8520)\n",
      "Total accuracy: 45.08%  (4012/8900)\n",
      "\n",
      "---------- Double Hard Biased ----------\n",
      "family.txt:\n",
      "ACCURACY TOP1: 72.63% (276/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 7.63% (71/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 14.15% (107/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 56.16% (748/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 39.77% (420/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 61.93% (654/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 45.45% (709/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 32.38% (408/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 54.74% (312/570)\n",
      "Questions seen/total: 46.25% (8900/19244)\n",
      "Semantic accuracy: 72.63%  (276/380)\n",
      "Syntactic accuracy: 40.25%  (3429/8520)\n",
      "Total accuracy: 41.63%  (3705/8900)\n"
     ]
    }
   ],
   "source": [
    "print('-'*10, 'Biased', '-'*10)\n",
    "evaluate_analogies(word_vectors)\n",
    "print()\n",
    "print('-'*10, 'Hard Biased', '-'*10)\n",
    "evaluate_analogies(new_vocab)\n",
    "print()\n",
    "print('-'*10, 'Double Hard Biased', '-'*10)\n",
    "evaluate_analogies(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujnFa05D90rb"
   },
   "source": [
    "Control:\n",
    "\n",
    "Embeddings given to us by the Black is to Criminal Paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qt4q_Hs09qGQ",
    "outputId": "cfbe20c8-a535-4c46-ac12-a6fff4b3024c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family.txt:\n",
      "ACCURACY TOP1: 70.26% (267/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 8.06% (75/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 17.86% (135/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 68.62% (914/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 40.91% (432/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 64.68% (683/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 48.85% (762/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 33.65% (424/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.14% (320/570)\n",
      "Questions seen/total: 46.25% (8900/19244)\n",
      "Semantic accuracy: 70.26%  (267/380)\n",
      "Syntactic accuracy: 43.96%  (3745/8520)\n",
      "Total accuracy: 45.08%  (4012/8900)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 72.37% (275/380)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 8.92% (83/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 19.31% (146/756)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 68.17% (908/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 40.91% (432/1056)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 63.07% (666/1056)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 50.19% (783/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 34.21% (431/1260)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 56.14% (320/570)\n",
      "Questions seen/total: 46.25% (8900/19244)\n",
      "Semantic accuracy: 72.37%  (275/380)\n",
      "Syntactic accuracy: 44.24%  (3769/8520)\n",
      "Total accuracy: 45.44%  (4044/8900)\n"
     ]
    }
   ],
   "source": [
    "evaluate_analogies(biased_word_vectors)\n",
    "evaluate_analogies(debiased_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE1-1Faf016M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Experiments Utility.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "db",
   "language": "python",
   "name": "db"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
